<!DOCTYPE html>
<html lang xmlLang><head><link rel="stylesheet" type="text/css" href="/experimentology-dev/assets/static/index.page.client.5ad503ce.css"><link rel="preload" href="/experimentology-dev/assets/static/SourceSansPro-Regular.71d10a86.ttf" as="font" type="font/ttf" crossorigin><meta charSet="utf-8"/><meta name="generator" content="pandoc"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta property="og:title" content="Chapter 10 Sampling | Experimentology"/><meta property="og:type" content="book"/><meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams"/><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><meta name="description" content="Chapter 10 Sampling | Experimentology"/><title>Chapter 10 Sampling | Experimentology</title><link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet"/><link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet"/><link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet"/><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet"/><style type="text/css">code{white-space: pre;}</style><style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style><style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style></head><body>



<div class="row">
<div class="col-sm-12">
<div island></div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="sampling" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Sampling</h1>
<div island></div>
<p>As we keep reminding you, experiments are designed to yield measurements of a causal effect. But a causal effect of what, and for whom? These are questions that are often given surprisingly little air time in our papers. Titles in our top journals read “Daxy thinking promotes fribbles,” “Doing fonzy improves smoodling,” or “Blicket practice produces more foozles than smonkers.”<label for="tufte-sn-142" class="margin-toggle sidenote-number">142</label><input type="checkbox" id="tufte-sn-142" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">142</span> Titles changed to protect the original authors. These researchers might very well have said more specific things in the text of their paper.</span> Each of these uses <strong>generic language</strong> to state a claim that is implied to be generally true <span class="citation">(<a href="#ref-dejesus2019" role="doc-biblioref">DeJesus et al. 2019</a>)</span>,<label for="tufte-sn-143" class="margin-toggle sidenote-number">143</label><input type="checkbox" id="tufte-sn-143" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">143</span> Generic language is a fascinating linguistic phenomenon. When we say things like “mosquitoes transmit malaria,” we don’t mean that <em>all</em> mosquitoes do it, only something like “it’s a valid and diagnostic generalization about mosquitoes in contrast to other relevant insects or other creatures that they are spreaders of malaria” <span class="citation">(<a href="#ref-tessler2019" role="doc-biblioref">Tessler and Goodman 2019</a>)</span>.</span> but for each of these, we could reasonably ask “doing fonzy improves smoodling <em>for whom</em>?” Is it everyone? Or a particular set of people? And similarly, we might want to ask “<em>how much</em> and <em>what kind</em> of fonzy reading?” These are questions about the <strong>generalizability of research</strong>.</p>
<p>Imagine for a second what a non-generic version of one of these titles might look like: “Reading one particular selection of fonzy for fifteen minutes in the lab improved 36 college students’ smoodling scores on a questionnaire.” We wouldn’t let the authors get away with a fully general version of their claim: “Doing [<em>any</em>] fonzy improves smoodling [<em>for anyone</em>].” That’s just a bad generalization from the evidence we actually have.</p>
<p>We’ve already run into generalizability in our treatment of statistical estimation and inference. When we estimated a particular quantity (say, the effect of fonzy), we did so in our own sample. But we then used inferential tools to reason about how the estimate in this <strong>sample</strong> related to the parameter in the <strong>population</strong> as a whole. How do we link up these <em>statistical</em> tools for generalization to the <em>scientific</em> questions we have about the generalizability of our findings? That’s the question of this chapter.</p>
<p>The first key set of decisions in experiment planning is what population to sample from and how to sample. We’ll start by talking about the basics of <strong>sampling theory</strong>: different ways of sampling and the generalizations they do and don’t license. The second section of the chapter will then deal with <strong>sampling biases</strong> that can compromise our effect estimates. A final set of key decisions is about <strong>sample size</strong> planning. In the third part of the chapter we’ll address this issue, starting with classic <strong>power analysis</strong> but then introduce several other ways that an experimenter can plan and justify their sample size.</p>
<div island></div>
<div id="sampling-theory" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Sampling theory</h2>
<p>The basic idea of sampling is simple: you want to estimate some measurement for a large or infinite population by measuring a sample from that population.<label for="tufte-sn-144" class="margin-toggle sidenote-number">144</label><input type="checkbox" id="tufte-sn-144" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">144</span> There are some tools for dealing with estimation in smaller populations where your sample is a substantial fraction of the population (e.g., a survey of your department where you get responses from half of the students). We won’t discuss those here; our focus is on generalizing to large populations of humans.</span> Sampling strategies are split into two categories: <strong>probability sampling</strong> – in which every member of the population has some chance of being selected – and <strong>non-probability sampling</strong> – in which there are some members of the population that simply cannot be selected. We’ll begin by discussing probability sampling, then we’ll talk about a useful technique for sampling called <strong>stratified sampling</strong>. With this discussion in hand, we’ll turn back to non-probability samples and their strengths and limitations.</p>
<div id="classical-probability-sampling" class="section level3" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> Classical probability sampling</h3>
<p>Classical sampling theory is built around the idea of <strong>probability sampling</strong>. There is some <strong>sampling frame</strong> containing every member of the population – think of a giant phone book with every adult human’s name in it. Then we use some kind of <strong>sampling strategy</strong>, maybe at the simplest just a completely random choice, to select <span class="math inline">\(N\)</span> humans from that sample frame, and then we collect our measure with them. This scenario is the one that informs all of our statistical results about how sample means converge to the population mean (as in Chapter <a href="6-inference.html#inference">6</a>).</p>
<p>Unfortunately, we very rarely do sampling of this sort in psychological research. Gathering true probability samples from the large populations that we’d like to generalize to is far too difficult and expensive. Consider the problems involved in doing some experiment with a sample of <em>all adult humans</em>, or even <em>adult English-speaking humans who are located in the United States</em>. As soon as you start to think about what it would take to collect a probability sample of this kind of population, the complexities get overwhelming. How will you find their names – what if they aren’t in the phone book? How will you contact them – what if they don’t have email? How will they do your experiment – what if they don’t have an up-to-date web browser? What if they don’t want to participate at all?</p>
<p>Instead, the vast majority of psychology research has been conducted with <strong>convenience samples</strong>: non-probability samples that feature individuals who can be recruited easily, such as college undergraduates or workers on crowdsourcing platforms like Amazon Mechanical Turk (see Chapter <a href="12-collection.html#collection">12</a>). We’ll turn to these below.</p>
<p>For survey research, on the other hand – think of election polling – there are many sophisticated techniques for dealing with sampling; although this field is still imperfect, it has advanced considerably in trying to predict complex and dynamic behaviors. One of the basic ideas is the construction of <strong>representative samples</strong>: samples that resemble the population in their representation of one or several sociodemographic characteristics like gender, income, race and ethnicity, age, political orientation.</p>
<p>Representative samples can be constructed by probability sampling, but they can also be constructed through non-probability methods like recruiting quotas of individuals from different groups. These methods are critical for much social science research, but they have been used less frequently in experimental psychology research and aren’t necessarily a critical part of the beginning experimentalist’s toolkit.<label for="tufte-sn-145" class="margin-toggle sidenote-number">145</label><input type="checkbox" id="tufte-sn-145" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">145</span> Readers can come up with counter-examples of recent studies that focus on representative sampling, but our guess is that they will prove the rule more generally. For example, a recent study tested the generality of growth mindset interventions for US high school students using a national sample <span class="citation">(<a href="#ref-yeager2019" role="doc-biblioref">Yeager et al. 2019</a>)</span>. This large-scale study sampled more than 100 high schools from a sampling frame of all registered high schools in the US, then randomly assigned students within schools that agreed to participate. They then checked that the schools that agreed to participate were representative of the broader population of schools. This study is great stuff, but we hope you agree that if you find yourself in this kind of situation – planning a multi-investigator 5 year consortium study on a national sample – you might want to consult with a statistician and not use an introductory book like this one.</span> There is one exception that we think you should know about, however.</p>
</div>
<div id="representative-samples-and-stratified-sampling" class="section level3" number="10.1.2">
<h3><span class="header-section-number">10.1.2</span> Representative samples and stratified sampling</h3>
<p>Imagine you’re interested in a particular measure in a population – say, attitudes towards tea drinking across US adults – but you think that this measure will vary with one or more characteristics such as whether the adults are frequent, infrequent, or non-coffee drinkers. Even worse, your measure might be more variable within one group: perhaps most frequent and infrequent coffee drinkers feel OK about tea, but as a group non-coffee drinkers tend to hate it (most don’t drink any caffeinated beverages).</p>
<p>A simple random sample from this heterogeneous population <em>will</em> converge asymptotically to the correct population average for tea-drinking attitudes. But it will do so more slowly than ideal because any given sample may over- or under-sample non-drinkers just by chance. In a small sample, if you happen to get too many non-coffee drinkers, your estimate of attitudes will be biased downward; if you happen to get too few, you will be biased upwards. All of this will come out in the wash eventually, but any individual sample (especially a small one) will be noisier than ideal.</p>
<div class="figure"><span style="display:block;" id="fig:sampling-stratified"></span>
<p class="caption marginnote shownote">
Figure 10.2: Illustration of stratified sampling. The left panel shows the sampling frame. The upper frames show the sampling frame stratified by a participant characteristic and a stratified sample. The lower frame shows a simple random sample, which happens to omit one group completely by chance.
</p>
<img src="images/sampling/stratified-sample2.png" alt="Illustration of stratified sampling. The left panel shows the sampling frame. The upper frames show the sampling frame stratified by a participant characteristic and a stratified sample. The lower frame shows a simple random sample, which happens to omit one group completely by chance." width="\linewidth"/>
</div>
<p><strong>Stratified sampling</strong> is a very useful trick that makes your effect estimates more precise in cases like this one <span class="citation">(<a href="#ref-neyman1992" role="doc-biblioref">Neyman 1992</a>)</span>. If you know the proportion of frequent, infrequent, or non-coffee drinkers in the population, you can sample within those subpopulations to ensure that your sample is representative along this dimension. This situation is pictured in Figure <a href="10-sampling.html#fig:sampling-stratified">10.2</a>, which shows how a particular sampling frame can be broken up into groups for stratified sampling (top). The result is a sample that matches the population proportions on a particular characteristic. In contrast, a simple random sample (bottom) can over- or under-sample the subgroups by chance.</p>
<p>
<span class="marginnote shownote">
<span style="display:block;" id="fig:sampling-stratified-sim"></span>
<img src="experimentology_files/figure-html/sampling-stratified-sim-1.png" alt="Simulation showing the potential benefits of stratification. Each dot is an estimated mean for a sample of a particular size, sampled randomly or with stratification. Red points show the mean and standard deviation of sample estimates." width="\linewidth"/>
Figure 10.3: Simulation showing the potential benefits of stratification. Each dot is an estimated mean for a sample of a particular size, sampled randomly or with stratification. Red points show the mean and standard deviation of sample estimates.
</span>
</p>
<p>Stratified sampling can lead to substantial gains in the precision of your estimate. These gains are most prominent when either the groups differ a lot in their mean or when they differ a lot in their variance.<label for="tufte-sn-146" class="margin-toggle sidenote-number">146</label><input type="checkbox" id="tufte-sn-146" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">146</span> There are several important refinements of stratified sampling in case you think these methods are important for your problem. In particular, <strong>optimal sampling</strong> can help you figure out how to over-sample groups with higher variance.</span> On the other hand, if the characteristic on which you stratify participants doesn’t relate to your measure at all, then stratified sampling converges just as fast as random sampling (though it’s a bit more of a pain to implement). Figure <a href="10-sampling.html#fig:sampling-stratified-sim">10.3</a> shows a simulation of the scenario in Figure <a href="10-sampling.html#fig:sampling-stratified">10.2</a>, in which each coffee preference group has a different tea attitude mean, and the smallest group has the biggest variance. Although the numbers here are invented, it’s clear that estimation error is much smaller in the stratified group and estimation error declines much more quickly as samples get larger.</p>
<p>Stratification is everywhere, and it’s useful even in convenience samples. For example, researchers who are interested in development typically stratify their samples across ages (e.g., recruiting equal numbers of two- and three-year-olds for a study of preschoolers). You can estimate developmental change in a pure random sample, but you are guaranteed good coverage of the range of interest when you stratify.</p>
<p>If you have a measure that you think varies with a particular characteristic, it’s not a bad idea to consider stratification. But don’t go overboard – you can drive yourself to distraction finding the last left-handed non-binary coffee drinker to complete your sample. Focus on stratifying when you know the measure varies with the characteristic of interest.</p>
</div>
</div>
<div id="convenience-samples-generalizability-and-the-weird-problem" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Convenience samples, generalizability, and the WEIRD problem</h2>
<p>Now let’s go back to the question of generalizability. How generalizable are the experimental effect estimates that we obtain in experiments that are conducted exclusively with convenience samples? We’ll start by laying out the worst version of the problem of generalizability in experimental psychology. We’ll then try to pull back from the brink and discuss some reasons why we might not want to be in despair despite some of the true generalizability issues that plague the psychology literature.</p>
<div id="the-worst-version-of-the-problem" class="section level3" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> The worst version of the problem</h3>
<p>Psychology is the study of the human mind. But from a sampling theory standpoint, not a single estimate in the published literature is based on a sample from the human population. So no estimate can be generalized to the object of study. And the situation is worse than that. Here are three of the most severe issues that have been raised regarding the generalizability of psychology research.</p>
<ol style="list-style-type:decimal;">
<li><p><strong>Convenience samples</strong>. Earlier, we dropped a little bombshell: almost all research in experimental psychology is performed with convenience samples. This issue has led to the remark that “the existing science of human behavior is largely the science of the behavior of sophomores” <span class="citation">(McNemar, 1946, quoted in <a href="#ref-rosenthal1984" role="doc-biblioref">Rosenthal and Rosnow 1984</a>)</span>. The samples we have easy access to just don’t represent the populations we want to describe! There is a twitter account devoted to finding biology papers that make big claims about curing diseases and appending the qualifier “in mice” to them. We might consider whether we need to do the same to psychology papers. Would “Doing fonzy improves smoodling <em>in sophomore college undergraduates in the Western US</em>” make it into a top journal?</p></li>
<li><p><strong>The WEIRD problem</strong>. Not only are the convenience samples that we study not representative of the local or national contexts in which they are recruited, those local and national contexts are also very far from the broader human experience (see <a href="1-experiments.html#experiments">1</a>). <span class="citation">Henrich, Heine, and Norenzayan (<a href="#ref-henrich2010" role="doc-biblioref">2010</a>)</span> coined the term WEIRD (Western, Educated, Industrialized, Rich, and Democratic) to sum up some of the ways that typical participants in psychology experiments differ from the typical human experience. Participants from WEIRD cultures likely think, speak, reason, and perceive differently than those from other backgrounds. The vast over-representation of WEIRD participants in the literature has led some researchers to suggest that published results simply reflect “WEIRD psychology” – a small and idiosyncratic part of a much broader universe of human psychology.<label for="tufte-sn-147" class="margin-toggle sidenote-number">147</label><input type="checkbox" id="tufte-sn-147" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">147</span> The term WEIRD has been very useful in drawing attention to the lack of representation of the breadth of human experiences in experimental psychology. But one negative consequence of this idea has been the response that what we need to do as a field is to sample more “non-WEIRD” people. The imposition of a binary distinction – in which every culture outside the WEIRD moniker is the same in some major respect – is unfair and not useful <span class="citation">(<a href="#ref-syed2020" role="doc-biblioref">Syed and Kathawalla 2020</a>)</span>. A better starting point is to consider the way that cultural variation might guide our choices about sampling.</span></p></li>
<li><p><strong>The item sampling issue</strong>. As we discussed in Chapter <a href="7-models.html#models">7</a>, we’re typically not just trying to generalize to new people, we’re also trying to generalize to new stimuli <span class="citation">(<a href="#ref-westfall2015" role="doc-biblioref">Westfall, Judd, and Kenny 2015</a>)</span>. The problem is that our experiments often use a very small set of stimuli, constructed by experimenters in an ad-hoc way rather than sampled as representatives of a broader population of stimuli that we hope to generalize to with our effect size estimate. What’s more, our statistical analyses sometimes fail to take stimulus variation into account (as discussed in Chapter <a href="7-models.html#models">7</a> and <a href="9-design.html#design">9</a>). Unless we know about the relationship of our stimuli to the broader population, our estimates may be based on unrepresentative samples in yet another way.</p></li>
</ol>
<p>In sum, experiments in the psychology literature primarily measure effects from WEIRD convenience samples of people and unsystematic samples of experimental stimuli. Should we throw up our hands and resign ourselves to an ungeneralizable “science” of sample-specific anecdotes?</p>
</div>
<div id="reasons-for-hope-and-ways-forward" class="section level3" number="10.2.2">
<h3><span class="header-section-number">10.2.2</span> Reasons for hope and ways forward</h3>
<p>We think the situation isn’t as bleak as the arguments above might have suggested. Underlying each of the arguments above is the notion of <strong>heterogeneity</strong>, the idea that particular experimental effects vary in the population.</p>
<p>Let’s think through a very simple version of this argument. Say we have an experiment that measures the smoodling effect, and it turns out that smoodling is completely universal and invariant throughout the human population. Now, if we want to get a precise estimate of smoodling, we can take <em>any</em> sample we want because everyone will show the same pattern. Because smoodling is homogeneous, a biased, non-representative sample will not cause problems. It turns out that there are likely tasks like this! For example, the Stroop task produces a consistent and similar interference effect for almost everyone <span class="citation">(<a href="#ref-hedge2018" role="doc-biblioref">Hedge, Powell, and Sumner 2018</a>)</span>.</p>
<div class="figure"><span style="display:block;" id="fig:sampling-heterogeneity"></span>
<p class="caption marginnote shownote">
Figure 10.4: Illustration of the interaction of heterogeneity and convenience samples. Left hand panels show sample composition. Individual plots show the weighted distribution of responses on some measure.
</p>
<img src="images/sampling/heterogeneity.png" alt="Illustration of the interaction of heterogeneity and convenience samples. Left hand panels show sample composition. Individual plots show the weighted distribution of responses on some measure." width="\linewidth"/>
</div>
<p>Figure <a href="10-sampling.html#fig:sampling-heterogeneity">10.4</a> illustrates this argument more broadly. If you have a random sample (top), then your sample mean and your population mean will converge to the same value, regardless of whether the effect is homogeneous (right) or heterogeneous (right). That’s the beauty of sampling theory.<label for="tufte-sn-148" class="margin-toggle sidenote-number">148</label><input type="checkbox" id="tufte-sn-148" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">148</span> You do get faster convergence if you stratify the sample, though – that’s precisely what our simulation above showed.</span> If you have a convenience sample (bottom), one part of the population is over-represented in the sample. The convenience sample doesn’t cause problems if the effect is homogeneous in the population – as with the case of smoodling or Stroop. The trouble comes when you have a heterogeneous effect. Because one group is over-represented, you get systematic bias in the sample mean relative to the population mean.</p>
<p>So the problems listed above – convenience samples, WEIRD samples, and narrow stimulus samples – only cause problems if effects are heterogeneous. Are they? The short answer is, <em>we don’t know</em>. Convenience samples are fine in the presence of homogeneous effects, but we only use convenience samples so we may not know which effects are homogeneous! Our metaphorical heads are in the sand.</p>
<p>We can’t do better than this circularity without a theory of what should be variable and what should be consistent between individuals.<label for="tufte-sn-149" class="margin-toggle sidenote-number">149</label><input type="checkbox" id="tufte-sn-149" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">149</span> Many people have theorized about the ways that culture and language in general might moderate psychological processes <span class="citation">(e.g., <a href="#ref-markus1991" role="doc-biblioref">Markus and Kitayama 1991</a>)</span>. What we’re talking about is related but slightly different – a theory not of what’s different, but of when there should be any difference and when there shouldn’t be. As an example, <span class="citation">Tsai (<a href="#ref-tsai2007" role="doc-biblioref">2007</a>)</span>’s “ideal affect” theory predicts that there should be more similarities in the distribution of actual affect across cultures, but that cultural differences should emerge in <em>ideal affect</em> (what people want to feel like) across cultures. This theory is a theory of when you should see homogeneity and when you should see heterogeneity.</span> As naïve observers of human behavior, differences between people often loom large. We are keen observers of social characteristics like age, gender, race, class, and education. For this reason, our first theories of psychology often foreground these characteristics as the primary locus for variation between people. Certainly these characteristics are important, but they fail to explain many of the <em>in</em>variances of human psychology as well. An alternative line of theorizing starts with the idea that “lower-level” parts of psychology – like perception – should be less variable than “higher-level” faculties like social cognition. This kind of theory sounds like a useful place to start, but there are also plenty of counter-examples in the literature <span class="citation">(<a href="#ref-henrich2010" role="doc-biblioref">Henrich, Heine, and Norenzayan 2010</a>)</span>.</p>
<p>Multi-lab, multi-nation studies can help to address questions about heterogeneity, breaking the circularity we described above and helping us get our heads out of the sand. For example, ManyLabs 2 systematically investigated replicability of a set of phenomena across cultures <span class="citation">(<a href="#ref-klein2018" role="doc-biblioref">O. Klein et al. 2018</a>)</span>, finding limited variation in effects between WEIRD sites and other sites. And in a study comparing a set of convenience and probability samples, <span class="citation">Coppock, Leeper, and Mullinix (<a href="#ref-coppock2018" role="doc-biblioref">2018</a>)</span> found limited demographic heterogeneity in another sample of experimental effects from across the social sciences. So there are at least some cases where we don’t have to worry as much about heterogeneity. More generally, large-scale studies like these offer the possibility of measuring and systematically characterizing demographic and cultural variation – as well as how variation itself varies between phenomena!</p>
</div>
</div>
<div id="sampling-bias" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Sampling bias</h2>
<p>In fields like econometrics or epidemiology that use observational methods to estimate causal effects, reasoning about <strong>sampling biases</strong> is a critical part of estimating generalizable effects. That’s because the measure that these fields are interested in – for example, outcomes to do with health and wealth – vary widely across individuals. So the way you sample can affect your estimated effect. In the kind of experimental work we are discussing, some of these issues are addressed by random assignment, but others are not.</p>
<div id="collider-bias" class="section level3" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Collider bias</h3>
<p>Imagine you want to measure the association between wealth and happiness through a (non-experimental) survey. As we discussed in Chapter <a href="1-experiments.html#experiments">1</a>, there are plenty of causal processes that could lead to this association. Figure <a href="10-sampling.html#fig:sampling-money">10.5</a> shows several of these scenarios. Money could truly cause happiness (1); happiness could cause you to make more money (2); or some third factor – say having lots of friends – could cause people to be happier <em>and</em> richer (3).</p>
<p>
<span class="marginnote shownote">
<span style="display:block;" id="fig:sampling-money"></span>
<img src="images/sampling/money3-drawing.png" alt="Four reasons why money and happiness can be correlated in a particular sample: 1. causal relationship, 2. reverse causality, 3. confounding with friendship, and 4. collider bias. For this last, we have to assume that our measurement is *conditioned* on being in this sample, meaning we only look at the association of money and happiness within the social services sample." width="\linewidth"/>
Figure 10.5: Four reasons why money and happiness can be correlated in a particular sample: 1. causal relationship, 2. reverse causality, 3. confounding with friendship, and 4. collider bias. For this last, we have to assume that our measurement is <em>conditioned</em> on being in this sample, meaning we only look at the association of money and happiness within the social services sample.
</span>
</p>
<p>But we can also create spurious associations if we are careless in our sampling. One prominent problem that we can induce is called <strong>collider bias</strong>. Suppose we recruited our sample from the clients of a social services agency. Unfortunately, both of our variables might have a causal relationship with presence in a social service agency (Figure <a href="10-sampling.html#fig:sampling-money">10.5</a>, 4): people might be interacting with the agency for financial or benefits assistance, or else for psychological services (perhaps due to depression). Being in a social services sample is called a <strong>collider</strong> variable because the two causal arrows <em>collide</em> into it (they both point to it).</p>
<p>If we look just within the social services sample, we might see a <em>negative</em> association between wealth and happiness – on average the people coming for financial assistance would have less wealth and more happiness than the people coming for psychological services. The take-home here is that if there is a causal relationship between your variables of interest and presence in your sample, you need to think carefully about whether you are inducing bias [sometimes, but not always, collider bias; <span class="citation">J. M. Rohrer (<a href="#ref-rohrer2018" role="doc-biblioref">2018</a>)</span>].</p>
<p>Random assignment still “works” in samples like this one – meaning, if you run a money intervention, you can still make an unbiased estimate of the effect of money on happiness. But doing your study in this sample raises a number of issues. First, the sample is heterogeneous on both baseline money and happiness, leading to problems with both the manipulation and the measure. A money manipulation may do more for participants who have less money and do less for others who have more, and a happiness measure may be at floor for some participants or ceiling for others. Second, just as we have discussed for other convenience samples, our estimate of the experimental effect may not generalize to individuals who don’t share characteristics with the convenience sample. So a money-happiness experiment using this population might still not be a good idea.</p>
</div>
<div id="bias-through-attrition" class="section level3" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Bias through attrition</h3>
<p><strong>Attrition</strong> is when people drop out of your study. You should do everything you can to improve participants’ experiences (see Chapter <a href="12-collection.html#collection">12</a>) but sometimes – especially when a manipulation is onerous for participants or an experiment requires longitudinal tracking – you will still have participants withdraw from the study.</p>
<p>Attrition on its own can be a threat to the generalizability of an experimental estimate. Imagine you do an experiment comparing a new very intense after-school math curriculum to a control curriculum in a sample of elementary school children over the course of a year. By the end of the year, suppose many of your participants have dropped out. The families who have stayed in the study are likely those who care most about math. Even if you see an effect of the curriculum intervention, this effect may generalize only to children in families who love math.</p>
<p>
<span class="marginnote shownote">
<span style="display:block;" id="fig:sampling-attrition"></span>
<img src="images/sampling/attrition-drawing.png" alt="Selective attrition can lead to a bias even in the presence of random assignment." width="\linewidth"/>
Figure 10.6: Selective attrition can lead to a bias even in the presence of random assignment.
</span>
</p>
<p>But there is a further problem with attrition. If one of your conditions leads to more attrition than the other, you can have a bias that confounds your conclusions, even in the presence of random assignment <span class="citation">(<a href="#ref-nunan2018" role="doc-biblioref">Nunan, Aronson, and Bankhead 2018</a>)</span>!
</p>
<p>Imagine students in the control condition of your math intervention experiment stayed in the sample, but the math intervention itself was so tough that most families dropped out. Now, when you compare math scores at the end of the experiment, you have a confound (Figure <a href="10-sampling.html#fig:sampling-attrition">10.6</a>): scores in the math condition could be higher simply because the families that remain in the study have a higher degree of interest in math.<label for="tufte-sn-150" class="margin-toggle sidenote-number">150</label><input type="checkbox" id="tufte-sn-150" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">150</span> Using a longitudinal design doesn’t help with this issue – the self-selected high math interest families could coach their kids on the curriculum, leading to greater growth over time even when controlling for baseline math scores.</span>
</p>
<p>You might think this kind of problem isn’t something we have to worry about, but it turns out that attrition bias can be pretty common even in short studies, especially when they are conducted online. And this bias can lead to false conclusions. For example, <span class="citation">Zhou and Fishbach (<a href="#ref-zhou2016" role="doc-biblioref">2016</a>)</span> ran an experiment in which they asked online participants to write about either 4 happy events (low difficulty) or 12 happy events (high difficulty) from the last year and then asked the participants to rate the difficulty of the task. Surprisingly, the high difficulty task was rated as easier than the low difficulty task! Selective attrition was the culprit for this counter-intuitive conclusion: while only 26% of participants dropped out of the low difficulty condition, a full 69% dropped out of the high difficulty task. The 31% that were left were so happy that it was actually quite easy for them to generate 12 happy events, and so they rated the objectively harder task as less difficult.</p>
<p>Always try to track and report attrition information. That lets you – and others – understand whether attrition poses threats to generalizability or even to causal inference more generally.<label for="tufte-sn-151" class="margin-toggle sidenote-number">151</label><input type="checkbox" id="tufte-sn-151" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">151</span> If you get interested in this topic, there is a whole field of statistics that focuses on <strong>missing data</strong> and that provides models for reasoning about and dealing with cases where data might not be <strong>missing completely at random</strong> <span class="citation">(<a href="#ref-little2019" role="doc-biblioref">Little and Rubin 2019</a> is the classic reference for these tools)</span>.</span></p>
</div>
</div>
<div id="sample-size-planning" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Sample size planning</h2>
<p>Now that you have spent some time considering your sample and what population it represents, how many people will your sample contain? As we learned in Chapter <a href="3-replication.html#replication">3</a>, continuing to collect data until you observe a <span class="math inline">\(p &lt; .05\)</span> in an inferential test is a good way to get a false positive. To avoid this situation, you need some kind of plan for when you will stop collecting data! Your <strong>stopping rule</strong> can then be a key part of your preregistration, so as to be transparent that your choice of stopping rule didn’t invalidate your statistical inference (see Chapter <a href="11-prereg.html#prereg">11</a>).</p>
<p>The simplest stopping rule is “I’ll collect data until I get to a target <span class="math inline">\(N\)</span>” – all that’s needed in this case is a value for <span class="math inline">\(N\)</span>. Classically, this value was computed using <strong>power analysis</strong>, which can provide a value for which you have a good chance of rejecting the null hypothesis (given a particular expected effect size).</p>
<p>Standard power analysis relies on knowing what size effect you are expecting, which is often an unrealistic assumption. If we knew what size effect we expected, wouldn’t we already have the information that we are conducting the experiment to get?</p>
<p><span class="marginnote shownote"><span id="tab:sampling-stopping-rules">Table 10.1: </span>Example classes of experimental stopping rules.</span></p>
<table><thead><tr><th style="text-align:right;">
</th><th style="text-align:left;">
Method
</th><th style="text-align:left;">
Stopping Rule
</th><th style="text-align:left;">
Example
</th></tr></thead><tbody><tr><td style="text-align:right;">
1
</td><td style="text-align:left;">
Power analysis
</td><td style="text-align:left;">
Stop at N for known probability of rejecting the null given known effect size
</td><td style="text-align:left;">
Randomized trial with strong expectations about effect size
</td></tr><tr><td style="text-align:right;">
2
</td><td style="text-align:left;">
Resource constraint
</td><td style="text-align:left;">
Stop collecting data after a certain amount of time or after a certain amount of resources are used
</td><td style="text-align:left;">
Time-limited field work
</td></tr><tr><td style="text-align:right;">
3
</td><td style="text-align:left;">
Smallest effect size of interest
</td><td style="text-align:left;">
Stop at N for known probability of rejecting the null for effects greater than some minimum
</td><td style="text-align:left;">
Measurement of unknown but important effect
</td></tr><tr><td style="text-align:right;">
4
</td><td style="text-align:left;">
Precision analysis
</td><td style="text-align:left;">
Stop at N that provides some known degree of precision in measure
</td><td style="text-align:left;">
Experimental measurement to compare with predictions of cognitive models
</td></tr><tr><td style="text-align:right;">
5
</td><td style="text-align:left;">
Sequential analysis
</td><td style="text-align:left;">
Stop when a known inferential criterion is reached
</td><td style="text-align:left;">
Intervention trial designed to accept or reject null with maximal efficiency
</td></tr></tbody></table>
<p>There are many different stopping rules that can be used to justify a sample size (Table <a href="10-sampling.html#tab:sampling-stopping-rules">10.1</a>). Each of these can provide a valid justification for a particular sample size, but they are most useful in different situations. We’ll first introduce classic power analysis both because it is a common standard and because it illustrates some of the concepts used in others.</p>
<div id="classic-power-analysis" class="section level3" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Classic power analysis</h3>
<p>
<span class="marginnote shownote">
<span style="display:block;" id="fig:sampling-neyman-pearson"></span>
<img src="images/sampling/power-alpha.png" alt="Standard decision matrix for NHST. The lower-left hand quadrant shows power to reject the null." width="\linewidth"/>
Figure 10.7: Standard decision matrix for NHST. The lower-left hand quadrant shows power to reject the null.
</span>
</p>
<p>Let’s start by reviewing the null-hypothesis significance testing paradigm that we introduced in Chapter <a href="6-inference.html#inference">6</a>. Recall that we introduced a decision-theoretic view of testing in Section 6.3.3, shown again in Figure <a href="10-sampling.html#fig:sampling-neyman-pearson">10.7</a>. The idea was that we’ve got some null hypothesis <span class="math inline">\(H_0\)</span> and some alternative <span class="math inline">\(H_1\)</span> – something like “no effect” and “yes, there is some effect with known size”– and we want to use data to decide which state we’re in. <span class="math inline">\(\alpha\)</span> is our criterion for rejecting the null, conventionally set to <span class="math inline">\(\alpha=.05\)</span>.</p>
<p>But what if <span class="math inline">\(H_0\)</span> is actually false and the alternative <span class="math inline">\(H_1\)</span> is true? Not all experiments are equally well set up to reject the null in those cases. Imagine doing an experiment with <span class="math inline">\(N=3\)</span>. In that case, we’d almost always fail to reject the null, even if it were false. Our sample would almost certainly be too small to rule out sampling variation as the source of our observed data.</p>
<p>Let’s try to quantify our willingness to <em>miss</em> the effect – the false negative rate. We’ll denote this probability with <span class="math inline">\(\beta\)</span>. If <span class="math inline">\(\beta\)</span> is the probability of missing an effect (failing to reject the null when it’s really false), then <span class="math inline">\(1-\beta\)</span> is the probability that we <em>correctly reject the null when it is false</em>. That’s what we call the <strong>statistical power</strong> of the experiment.</p>
<p>We can only compute power if we know the effect size for the alternative hypothesis. If the alternative hypothesis is a small effect, then the probability of rejecting the null will typically be low (unless the sample size is very large). In contrast, if the alternative hypothesis is a large effect, then the probability of rejecting the null will be higher.</p>
<div class="figure"><span style="display:block;" id="fig:sampling-power"></span>
<p class="caption marginnote shownote">
Figure 10.8: Illustration of how larger sample sizes lead to greater power.
</p>
<img src="images/sampling/power.png" alt="Illustration of how larger sample sizes lead to greater power." width="\linewidth"/>
</div>
<p>The same dynamic holds with sample size: the same effect size will be easier to detect with a larger sample size than with a small one. Figure <a href="10-sampling.html#fig:sampling-power">10.8</a> shows how this relationship works. A large sample size creates a tighter null distribution (right side) by reducing sampling error. A tighter null distribution means you can reject the null more of the time based on the variation in a true effect. If your sample size is too small to detect your effect much of the time, we call this being <strong>under-powered</strong>.<label for="tufte-sn-152" class="margin-toggle sidenote-number">152</label><input type="checkbox" id="tufte-sn-152" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">152</span> You can also refer to a design as <strong>over-powered</strong>, though we object slightly to this characterization, since the value of large datasets is typically not just to reject the null but also to measure an effect with high precision and to investigate how it is moderated by other characteristics of the sample.</span></p>
<p>Classical power analysis involves computing the sample size <span class="math inline">\(N\)</span> that’s necessary in order to achieve some level of power, given <span class="math inline">\(\alpha\)</span> and a known effect size.<label for="tufte-sn-153" class="margin-toggle sidenote-number">153</label><input type="checkbox" id="tufte-sn-153" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">153</span> Our focus here is on giving you a conceptual introduction to power analysis, but we refer you to <span class="citation">Cohen (<a href="#ref-cohen1992" role="doc-biblioref">1992</a>)</span> for a more detailed introduction.</span> The mathematics of the relationship between <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(N\)</span>, and effect size have been worked out for a variety of different statistical tests <span class="citation">(<a href="#ref-cohen2013" role="doc-biblioref">Cohen 2013</a>)</span> and codified in software like G*Power <span class="citation">(<a href="#ref-faul2007" role="doc-biblioref">Faul et al. 2007</a>)</span> and the <code>pwr</code> package for R <span class="citation">(<a href="#ref-champely2017" role="doc-biblioref">Champely et al. 2017</a>)</span>. For other cases (including mixed effects models), you may have to conduct a simulation in which you generate many simulated experimental runs under known assumptions and compute how many of these lead to a significant effect; luckily, R packages exist for this purpose as well, including the <code>simr</code> package <span class="citation">(<a href="#ref-green2016" role="doc-biblioref">Green and MacLeod 2016</a>)</span>.</p>
</div>
<div id="power-analysis-in-practice" class="section level3" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> Power analysis in practice</h3>
<p>Let’s do a power analysis for our hypothetical money and happiness intervention. Imagine the experiment is a simple two group design in which participants from a convenience population are randomly assigned either to receive $1000 and some advice on saving money (experimental condition) vs. just receiving the advice and no money (control condition). We then follow up a month later and collect self-reported happiness ratings. How many people should we have in our study in order to be able to reject the null? The answer to this question depends on our desired values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> as well as our expected effect size for the intervention.</p>
<p>For <span class="math inline">\(\alpha\)</span> we will just set a conventional significance threshold of <span class="math inline">\(\alpha = .05\)</span>. But what should be our desired level of power? It’s been standard in the social sciences to aim for power above 80% (e.g., <span class="math inline">\(\beta &lt; .20\)</span>); this gives you 4 out of 5 chances to observe a significant effect. But just like <span class="math inline">\(alpha = .05\)</span>, this is a conventional value that is perhaps a little bit too loose for modern standards – a strong test of a particular effect should probably have 90% or 95% power.<label for="tufte-sn-154" class="margin-toggle sidenote-number">154</label><input type="checkbox" id="tufte-sn-154" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">154</span> Really, researchers interested in using power analysis in their work should give some thought to what sort of chance of a false negative they are willing to accept. In exploratory research perhaps a higher chance of missing an effect is reasonable; in contrast, in confirmatory research it might make sense to aim for a higher level of power.</span></p>
<p>These choices are relatively easy, compared to the fundamental issue: our power analysis requires some expectation about the effect size for our intervention! This is the <strong>first fundamental problem of power analysis</strong>: if you knew the effect size, you might not need to do the experiment!</p>
<p>So how are you supposed to get an estimate of effect size? Here are a few possibilities:</p>
<ul>
<li><p><strong>Meta-analysis</strong>. If there is a good meta-analysis of the effect that you are trying to measure (or something closely related), then you are in luck. A strong meta-analysis will have not only a precise effect size estimate but also some diagnostics detecting and correcting potential publication bias in the literature (see Chapter <a href="16-meta.html#meta">16</a>). In this best-case scenario, you can use the meta-analytic effect size estimate as the basis for a power analysis.</p></li>
<li><p><strong>Specific prior study</strong>. A more complicated scenario is when you have only one or a handful of prior studies that you would like to use as a guide. The trouble is that any individual effect in the literature is likely subject to publication bias (see Chapter <a href="3-replication.html#replication">3</a>). Thus, using this estimate likely means your study will be under-powered – you might not get as lucky as a previous study did!</p></li>
<li><p><strong>Pilot testing</strong>. Many people (including us) at some point learned that one way to do a power analysis is to conduct a pilot study, estimate the effect size from the pilot, and then use this effect estimate for power analysis in the main study. <em>Do not do this</em>. The trouble is that your pilot study will have a small sample size, leading to a very imprecise estimate of effect size <span class="citation">(<a href="#ref-browne1995" role="doc-biblioref">Browne 1995</a>)</span>. If you over-estimate the effect size, your main study will be very under-powered. If you under-estimate, the opposite will be true. Using a pilot for power analysis is a recipe for problems.</p></li>
<li><p><strong>General expectations about an effect of interest</strong>. In our view, perhaps the best way you can use power analysis (in the absence of a really strong meta-analysis, at least) is to start with a general idea about the size of effect you expect and would like to be able to detect. It is totally reasonable to say, “I don’t know how big my effect is going to be, but let’s see what my power would be if it were <em>medium-sized</em> (say <span class="math inline">\(d=.5\)</span>), since that’s the kind of thing we’re hoping for with our money intervention.” This kind of power analysis can help you set your expectations about what range of effects you might be able to detect with a given sample size.</p></li>
</ul>
<p>For our money study, using our general expectation of a medium size effect, we can compute power for <span class="math inline">\(d=.5\)</span>. In this case, we’ll simply use the two-sample <span class="math inline">\(t\)</span>-test introduced in Chapter <a href="6-inference.html#inference">6</a>, for which 80% power at <span class="math inline">\(\alpha = .05\)</span> and <span class="math inline">\(d=.5\)</span> is achieved by having <span class="math inline">\(N=64\)</span> in each group.</p>
<div island></div>
<p>There’s a second issue, however. The <strong>second fundamental problem of power analysis</strong> is that the real effect size for an experiment may be zero. And in that case, <em>no</em> sample size will let you correctly reject the null. Going back to our discussion in Chapter <a href="6-inference.html#inference">6</a>, the null hypothesis significance testing framework is just not set up to let you <em>accept</em> the null hypothesis. If you are interested in a bi-directional approach to hypothesis testing in which you can accept <em>and</em> reject the null, you may need to consider Bayesian approaches or <strong>equivalence testing</strong> approaches <span class="citation">(<a href="#ref-lakens2018" role="doc-biblioref">Lakens, Scheel, and Isager 2018</a>)</span>, which do not fit the assumptions of classical power analysis.</p>
</div>
<div id="alternative-approaches-to-sample-size-planning" class="section level3" number="10.4.3">
<h3><span class="header-section-number">10.4.3</span> Alternative approaches to sample size planning</h3>
<p>Let’s now consider some alternatives to classic power analysis that can still yield reasonable sample size justifications.</p>
<ol style="list-style-type:decimal;">
<li><p><strong>Resource constraint</strong>. In some cases, there are fundamental resource constraints that limit data collection. For example, if you are doing fieldwork, sometimes the right stopping criterion for data collection is “when the field visit is over,” since every additional datapoint is valuable. When pre-specified, these kinds of sample size justifications can be quite reasonable, although they do not preclude being under-powered to test a particular hypothesis.</p></li>
<li><p><strong>Smallest effect size of interest (SESOI)</strong>. SESOI analysis is a variant on power analysis that includes some resource constraint planning. Instead of trying to intuit how big your target effect is, you instead choose a level below which you might not be interested in detecting the effect. This choice can be informed by how expensive or time-consuming it might be to test for a very small effect with high power. In practice, SESOI analysis is as simple as conducting a classic power analysis with a particular small effect as the target.</p></li>
<li><p><strong>Precision-based sample planning</strong>. As we discussed in Chapter <a href="6-inference.html#inference">6</a>, the goal of research is not always to reject the null hypothesis! Sometimes – we’d argue that it should be most of the time – the goal is to estimate a particular causal effect of interest with a high level of precision, since these estimates are a prerequisite for building theories. If what you want is an estimate with known precision (say, a confidence interval of a particular width), you can compute the sample size necessary to achieve that precision <span class="citation">(<a href="#ref-bland2009" role="doc-biblioref">Bland 2009</a>; <a href="#ref-rothman2018" role="doc-biblioref">Rothman and Greenland 2018</a>)</span>.<label for="tufte-sn-155" class="margin-toggle sidenote-number">155</label><input type="checkbox" id="tufte-sn-155" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">155</span> In our experience, this kind of planning is most useful when you are attempting to gather measurements with sufficient precision to compare between computational models. Since the models can make quantitative predictions that differ by some known amount, then it’s clear how tight your confidence intervals need to be.</span></p></li>
<li><p><strong>Sequential analysis</strong>. Your stopping rule need not be a hard cutoff at a specific <span class="math inline">\(N\)</span>. Instead, it’s possible to plan a <strong>sequential analysis</strong> using either frequentist or Bayesian methods, in which you plan to stop collecting data once a particular result is reached. For the frequentist version, the key thing that keeps sequential analysis from being <span class="math inline">\(p\)</span>-hacking is that you pre-specify particular values of <span class="math inline">\(N\)</span> at which you will conduct tests and then correct your <span class="math inline">\(p\)</span>-values for having tested multiple times <span class="citation">(<a href="#ref-lakens2014" role="doc-biblioref">Lakens 2014</a>)</span>. For Bayesian sequential analysis, you can actually compute a running Bayes factor as you collect data and stop when you reach a pre-specified level of evidence <span class="citation">(<a href="#ref-schonbrodt2017" role="doc-biblioref">Schönbrodt et al. 2017</a>)</span>. This latter alternative has the advantage of allowing you to collect evidence <em>for</em> the null as well as against it.<label for="tufte-sn-156" class="margin-toggle sidenote-number">156</label><input type="checkbox" id="tufte-sn-156" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">156</span> Another interesting variant is sequential parameter estimation, in which you collect data until a desired level of precision is achieved <span class="citation">(<a href="#ref-kelley2018" role="doc-biblioref">Kelley, Darku, and Chattopadhyay 2018</a>)</span>; this approach combines some of the benefits of both precision-based analysis and sequential analysis.</span></p></li>
</ol>
<p>In sum, there are many different ways of justifying your sample size or your stopping rule. The most important things are 1) to pre-specify your strategy and 2) to give a clear justification for your choice. Table <a href="10-sampling.html#tab:sampling-justification">10.2</a> gives an example sample size justification that draws on several different concepts discussed here, using classical power computations as one part of the justification. A reviewer could easily follow the logic of this discussion and form their own conclusion about whether this study had an adequate sample size and whether it should have been conducted given the researchers’ constraints.</p>
<p><span class="marginnote shownote"><span id="tab:sampling-justification">Table 10.2: </span>Example sample size justification, referencing elements of SESOI, resource-limitation, and power-based reasoning.</span></p>
<table><thead><tr><th style="text-align:left;">
Element
</th><th style="text-align:left;">
Justification Text
</th></tr></thead><tbody><tr><td style="text-align:left;">
Background
</td><td style="text-align:left;">
We did not have strong prior information about the likely effect size, so we could not compute a classical power analysis.
</td></tr><tr><td style="text-align:left;">
Smallest effect of interest
</td><td style="text-align:left;">
Because of our interest in meaningful factors affecting word learning, we were interested in effect sizes as small as d=.5.
</td></tr><tr><td style="text-align:left;">
Resource limitation
</td><td style="text-align:left;">
We were also limited by our ability to collect data only at our on-campus preschool.
</td></tr><tr><td style="text-align:left;">
Power computation
</td><td style="text-align:left;">
We calculated that based on our maximal possible sample size of N=120 (60 per group), we would achieve at least 80% power to reject the null for effects as small as d = .52.
</td></tr></tbody></table>
<div island></div>
</div>
</div>
<div id="chapter-summary-sampling" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Chapter summary: Sampling</h2>
<p>Your goal as an experimenter is to estimate a causal effect. But the effect for whom? This chapter has tried to help you think about how you generalize from your experimental sample to some target population. It’s very rare to be conducting an experiment based on a probability sample in which every member of the population has an equal chance of being selected. In the case that you are using a convenience sample, you will need to consider how bias introduced by the sample could relate to the effect estimate you observed. Do you think this effect is likely to be very heterogeneous in the population? Are there theories that suggest that it might be larger or smaller for the convenience sample you recruited?</p>
<p>Questions about generalizability and sampling depend on the precise construct you are studying, and there is no mechanistic procedure for answering them. Instead, you simply have to ask yourself: how does my sampling procedure qualify the inference I want to make based on my data? Being transparent about your reasoning can be very helpful – both to you and to readers of your work who want to contextualize the generality of your findings.</p>
<div island></div>
<div island></div>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bland2009" class="csl-entry">
Bland, John Martin. 2009. <span>“The Tyranny of Power: Is There a Better Way to Calculate Sample Size?”</span> <em>BMJ</em> 339 (October): b3985.
</div>
<div id="ref-browne1995" class="csl-entry">
Browne, Richard H. 1995. <span>“On the Use of a Pilot Sample for Sample Size Determination.”</span> <em>Statistics in Medicine</em> 14 (17): 1933–40.
</div>
<div id="ref-champely2017" class="csl-entry">
Champely, Stephane, Claus Ekstrom, Peter Dalgaard, Jeffrey Gill, Stephan Weibelzahl, Aditya Anandkumar, Clay Ford, Robert Volcic, and Helios De Rosario. 2017. <span>“Pwr: Basic Functions for Power Analysis.”</span>
</div>
<div id="ref-cohen1992" class="csl-entry">
———. 1992. <span>“A Power Primer.”</span> <em>Psychological Bulletin</em> 112 (1): 155.
</div>
<div id="ref-cohen2013" class="csl-entry">
———. 2013. <em>Statistical Power Analysis for the Behavioral Sciences</em>. Routledge.
</div>
<div id="ref-coppock2018" class="csl-entry">
Coppock, Alexander, Thomas J Leeper, and Kevin J Mullinix. 2018. <span>“Generalizability of Heterogeneous Treatment Effect Estimates Across Samples.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (49): 12441–46.
</div>
<div id="ref-dejesus2019" class="csl-entry">
DeJesus, Jasmine M, Maureen A Callanan, Graciela Solis, and Susan A Gelman. 2019. <span>“Generic Language in Scientific Communication.”</span> <em>Proceedings of the National Academy of Sciences</em> 116 (37): 18370–77.
</div>
<div id="ref-faul2007" class="csl-entry">
Faul, Franz, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007. <span>“G* Power 3: A Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences.”</span> <em>Behavior Research Methods</em> 39 (2): 175–91.
</div>
<div id="ref-green2016" class="csl-entry">
Green, Peter, and Catriona J MacLeod. 2016. <span>“SIMR: An r Package for Power Analysis of Generalized Linear Mixed Models by Simulation.”</span> <em>Methods in Ecology and Evolution</em> 7 (4): 493–98.
</div>
<div id="ref-hedge2018" class="csl-entry">
Hedge, Craig, Georgina Powell, and Petroc Sumner. 2018. <span>“The Reliability Paradox: Why Robust Cognitive Tasks Do Not Produce Reliable Individual Differences.”</span> <em>Behavior Research Methods</em> 50 (3): 1166–86.
</div>
<div id="ref-henrich2010" class="csl-entry">
Henrich, Joseph, Steven J Heine, and Ara Norenzayan. 2010. <span>“The Weirdest People in the World?”</span> <em>Behavioral and Brain Sciences</em> 33 (2-3): 61–83.
</div>
<div id="ref-kelley2018" class="csl-entry">
Kelley, Ken, Francis Bilson Darku, and Bhargab Chattopadhyay. 2018. <span>“Accuracy in Parameter Estimation for a General Class of Effect Sizes: A Sequential Approach.”</span> <em>Psychological Methods</em> 23 (2): 226.
</div>
<div id="ref-klein2018" class="csl-entry">
Klein, Olivier, Tom E Hardwicke, Frederik Aust, Johannes Breuer, Henrik Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf Vanpaemel, and Michael C Frank. 2018. <span>“A Practical Guide for Transparency in Psychological Science.”</span>
</div>
<div id="ref-lakens2014" class="csl-entry">
———. 2014. <span>“Performing High-Powered Studies Efficiently with Sequential Analyses.”</span> <em>European Journal of Social Psychology</em> 44 (7): 701–10.
</div>
<div id="ref-lakens2018" class="csl-entry">
Lakens, Daniël, Anne M. Scheel, and Peder M. Isager. 2018. <span>“Equivalence Testing for Psychological Research: A Tutorial.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (2): 259–69. <a href="https://doi.org/10.1177/2515245918770963">https://doi.org/10.1177/2515245918770963</a>.
</div>
<div id="ref-little2019" class="csl-entry">
Little, Roderick JA, and Donald B Rubin. 2019. <em>Statistical Analysis with Missing Data</em>. Vol. 793. John Wiley &amp; Sons.
</div>
<div id="ref-majid2014" class="csl-entry">
Majid, Asifa, and Niclas Burenhult. 2014. <span>“Odors Are Expressible in Language, as Long as You Speak the Right Language.”</span> <em>Cognition</em> 130 (2): 266–70.
</div>
<div id="ref-majid2018" class="csl-entry">
Majid, Asifa, and Nicole Kruspe. 2018. <span>“Hunter-Gatherer Olfaction Is Special.”</span> <em>Current Biology</em> 28 (3): 409–13.
</div>
<div id="ref-markus1991" class="csl-entry">
Markus, Hazel R, and Shinobu Kitayama. 1991. <span>“Culture and the Self: Implications for Cognition, Emotion, and Motivation.”</span> <em>Psychological Review</em> 98 (2): 224.
</div>
<div id="ref-neyman1992" class="csl-entry">
Neyman, Jerzy. 1992. <span>“On the Two Different Aspects of the Representative Method: The Method of Stratified Sampling and the Method of Purposive Selection.”</span> In <em>Breakthroughs in Statistics</em>, 123–50. Springer.
</div>
<div id="ref-nosek2021" class="csl-entry">
Nosek, Brian A, Tom E Hardwicke, Hannah Moshontz, Aurélien Allard, Katherine S Corker, Anna Dreber Almenberg, Fiona Fidler, et al. 2021. <span>“Replicability, Robustness, and Reproducibility in Psychological Science.”</span> <em>Annual Review of Psychology</em>.
</div>
<div id="ref-nunan2018" class="csl-entry">
Nunan, David, Jeffrey Aronson, and Clare Bankhead. 2018. <span>“Catalogue of Bias: Attrition Bias.”</span> <em>BMJ Evidence-Based Medicine</em> 23 (1): 21–22.
</div>
<div id="ref-piantadosi2014" class="csl-entry">
Piantadosi, Steven T, and Edward Gibson. 2014. <span>“Quantitative Standards for Absolute Linguistic Universals.”</span> <em>Cognitive Science</em> 38 (4): 736–56.
</div>
<div id="ref-rohrer2018" class="csl-entry">
Rohrer, Julia M. 2018. <span>“Thinking Clearly about Correlations and Causation: Graphical Causal Models for Observational Data.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (1): 27–42.
</div>
<div id="ref-rosenthal1984" class="csl-entry">
Rosenthal, Robert, and Ralph L Rosnow. 1984. <em>Essentials of Behavioral Research: Methods and Data Analysis</em>. New York: McGraw-Hill.
</div>
<div id="ref-rothman2018" class="csl-entry">
Rothman, Kenneth J, and Sander Greenland. 2018. <span>“Planning Study Size Based on Precision Rather Than Power.”</span> <em>Epidemiology</em> 29 (5): 599–603.
</div>
<div id="ref-schonbrodt2017" class="csl-entry">
Schönbrodt, Felix D, Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. 2017. <span>“Sequential Hypothesis Testing with Bayes Factors: Efficiently Testing Mean Differences.”</span> <em>Psychol. Methods</em> 22 (2): 322–39.
</div>
<div id="ref-simonsohn2015" class="csl-entry">
———. 2015. <span>“Small Telescopes: Detectability and the Evaluation of Replication Results.”</span> <em>Psychol. Sci.</em> 26 (5): 559–69.
</div>
<div id="ref-syed2020" class="csl-entry">
Syed, Moin, and U Kathawalla. 2020. <span>“Cultural Psychology, Diversity, and Representation in Open Science.”</span> <em>Cultural Methods in Psychology: Describing and Transforming Cultures</em>, 427–54.
</div>
<div id="ref-tessler2019" class="csl-entry">
Tessler, Michael Henry, and Noah D Goodman. 2019. <span>“The Language of Generalization.”</span> <em>Psychological Review</em> 126 (3): 395.
</div>
<div id="ref-tsai2007" class="csl-entry">
Tsai, Jeanne L. 2007. <span>“Ideal Affect: Cultural Causes and Behavioral Consequences.”</span> <em>Perspectives on Psychological Science</em> 2 (3): 242–59.
</div>
<div id="ref-westfall2015" class="csl-entry">
Westfall, Jacob, Charles M Judd, and David A Kenny. 2015. <span>“Replicating Studies in Which Samples of Participants Respond to Samples of Stimuli.”</span> <em>Perspectives on Psychological Science</em> 10 (3): 390–99.
</div>
<div id="ref-yeager2019" class="csl-entry">
Yeager, David S, Paul Hanselman, Gregory M Walton, Jared S Murray, Robert Crosnoe, Chandra Muller, Elizabeth Tipton, et al. 2019. <span>“A National Experiment Reveals Where a Growth Mindset Improves Achievement.”</span> <em>Nature</em> 573 (7774): 364–69.
</div>
<div id="ref-zhou2016" class="csl-entry">
Zhou, Haotian, and Ayelet Fishbach. 2016. <span>“The Pitfall of Experimenting on the Web: How Unattended Selective Attrition Leads to Surprising (yet False) Research Conclusions.”</span> <em>Journal of Personality and Social Psychology</em> 111 (4): 493.
</div>
</div>
<p style="text-align:center;">
<a href="9-design.html"><button class="btn btn-default">Previous</button></a>
<a href="11-prereg.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



<script type="module" src="/experimentology-dev/assets/entries/entry-server-routing.21024ed5.js" defer></script><link rel="modulepreload" href="/experimentology-dev/assets/entries/src_index.page.client.79e072fd.js" as="script" type="text/javascript"><link rel="modulepreload" href="/experimentology-dev/assets/chunks/chunk-7bd171af.js" as="script" type="text/javascript"><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index","islands":[{"name":"TOC","props":{"name":"Experimentology: An Open Science Approach to Experimental Psychology Methods","items":[{"name":"Preliminaries","items":[{"name":"Experiments","href":"1-experiments"},{"name":"Theories","href":"2-theories"},{"name":"Replication","href":"3-replication"},{"name":"Ethics","href":"4-ethics"}]},{"name":"Statistics","items":[{"name":"Estimation","href":"5-estimation"},{"name":"Inference","href":"6-inference"},{"name":"Models","href":"7-models"}]},{"name":"Design","items":[{"name":"Measurement","href":"8-measurement"},{"name":"Design","href":"9-design"},{"name":"Sampling","href":"10-sampling"}]},{"name":"Execution","items":[{"name":"Preregistration","href":"11-prereg"},{"name":"Data collection","href":"12-collection"},{"name":"Project management","href":"13-management"}]},{"name":"Reporting","items":[{"name":"Writing","href":"14-writing"},{"name":"Visualization","href":"15-viz"},{"name":"Meta-analysis","href":"16-meta"},{"name":"Conclusions","href":"17-conclusions"}]},{"name":"Appendices","items":[{"name":"GitHub","href":"A-git"},{"name":"R Markdown","href":"B-rmarkdown"},{"name":"Tidyverse","href":"C-tidyverse"},{"name":"ggplot","href":"D-ggplot"},{"name":"Instructor’s guide","href":"E-instructors"}]}]}},{"name":"Box","props":{"children":["\n",{"type":"ul","props":{"children":["\n",{"type":"li","props":{"children":"Discuss sampling theory and stratified sampling"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6052},"\n",{"type":"li","props":{"children":"Reason about the limitations of different samples, especially convenience samples"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6053},"\n",{"type":"li","props":{"children":"Consider sampling biases and how they affect your inferences"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6054},"\n",{"type":"li","props":{"children":"Learn how to choose and justify an appropriate sample size for your experiment"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6055},"\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6056},"\n"],"className":"box","data-box":"learning_goals"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["Since Darwin, scientists have assumed that smell is a vestigial sense in humans – one that we don’t even bother to encode in language. In English we don’t even have consistent words for odors. We can say something is “stinky,” “fragrant, or maybe”musty,” but beyond these, all our words for smells are about the ",{"type":"em","props":{"children":"source"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6090}," of the smell, not the qualities of it. Bananas, roses, and skunks all have distinctive smells, but we don’t have any vocabulary for naming what is common or uncommon about them. And when we make up ad-hoc vocabulary, it’s typically quite inconsistent ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-majid2014","role":"doc-biblioref","children":"Majid and Burenhult 2014"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6091},")"]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6092},". The same situation applies across many languages."]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6093},"\n",{"type":"p","props":{"children":"So, would it be a good generalization about human beings – all people – that olfaction as a sense is de-emphasized relative to vision? This inference has a classic sample-to-population structure. Within several samples of participants using widely-spoken languages, we observe limited and inconsistent vocabulary for smells, as well as poor discrimination. We use these samples to license an inference to the population – in this case, the entire human population."},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6094},"\n",{"type":"p","props":{"children":["\n",{"type":"span","props":{"className":"marginnote shownote","children":["\n",{"type":"span","props":{"style":{"display":"block"},"id":"fig:sampling-majid2014","children":null},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6095},"\n",{"type":"img","props":{"src":"images/sampling/majid2014.png","alt":"Data from Majid and Burenhult (2014) on the consistency of color and odor naming in English and Jahai speakers. Higher values indicate more consistent descriptions. Pie charts indicate the type of language being used.","width":"\\linewidth","children":null},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6096},"\n","Figure 10.1: Data from Majid and Burenhult (2014) on the consistency of color and odor naming in English and Jahai speakers. Higher values indicate more consistent descriptions. Pie charts indicate the type of language being used.","\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6097},"\n"]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6098},"\n",{"type":"p","props":{"children":["But these inferences about the universal lack of olfactory vocabulary are likely based on choosing the wrong samples! Multiple hunter-gatherer groups appear to have large vocabularies for consistent smell description. For example, the Jahai, a hunter-gatherer group on the Malay Peninsula, have a vocabulary that includes at least twelve words for distinct odors, for example /cŋεs/, which names odors with a “stinging smell” like gasoline, smoke, or bat droppings. When Jahai speakers are asked to name odors, they produce shorter and much more consistent descriptions than English speakers – in fact, their smell descriptions were as consistent as their color descriptions (Figure ",{"type":"a","props":{"href":"10-sampling.html#fig:sampling-majid2014","children":"10.1"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6099},"). Further studies implicate the hunter-gatherer lifestyle as a factor: while several hunter-gatherer groups show good odor naming, nearby horticulturalist groups don’t ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-majid2018","role":"doc-biblioref","children":"Majid and Kruspe 2018"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6100},")"]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6101},"."]},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6102},"\n",{"type":"p","props":{"children":["Generalizations about humans are tricky. If you want to estimate the average odor naming ability, you could take a random sample of humans and evaluate their odor naming. Most of the individuals in the sample would likely speak English, Mandarin, Hindi, or Spanish. Almost certainly, none of them would speak Jahai, which is spoken by only a little more than a thousand people and is listed as ",{"type":"a","props":{"href":"https://www.ethnologue.com/language/jhi","children":"Threatened"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6103}," by Ethnologue. Your estimate of low odor naming stability would be a good guess for the ",{"type":"em","props":{"children":"majority"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6104}," of the world’s population."]},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6105},"\n",{"type":"p","props":{"children":["On the other hand, it’s more complicated to jump from a statistical generalization about average ability to a richer claim, like “as humans evolved, they lost olfactory ability and gained visual ability.” Such claims about universal aspects of the human experience require much more care and much stronger evidence ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-piantadosi2014","role":"doc-biblioref","children":"Piantadosi and Gibson 2014"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6106},")"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6107},". From a sampling perspective, human behavior and cognition show immense and complex ",{"type":"strong","props":{"children":"heterogeneity"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6108}," – variability of individuals and variability across clusters. Put simply, if we want to know what people in general are like, we have to think carefully about which people we include in our studies."]},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6109},"\n"],"className":"box","data-box":"case_study","data-title":"Is everyone as bad at describing smells as I am? "}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["Classic power analysis in R is quite simple using the ",{"type":"code","props":{"children":"pwr"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6463}," package. The package offers a set of test-specific functions like ",{"type":"code","props":{"children":"pwr.t.test"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6464},". For each, you supply three of the four parameters specifying effect size (",{"type":"code","props":{"children":"d"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6465},"), number of observations (",{"type":"code","props":{"children":"n"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6466},"), significance level (",{"type":"code","props":{"children":"sig.level"},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6467},"), and power (",{"type":"code","props":{"children":"power"},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6468},"); the function computes the fourth. For classic power analysis, we leave out ",{"type":"code","props":{"children":"n"},"key":13,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6469},":"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6470},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb19","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":[{"type":"span","props":{"id":"cb19-1","children":[{"type":"a","props":{"href":"10-sampling.html#cb19-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6471},{"type":"span","props":{"className":"fu","children":"pwr.t.test"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6472},"(",{"type":"span","props":{"className":"at","children":"d ="},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6473}," .",{"type":"span","props":{"className":"dv","children":"5"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6474},", "]},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6475},"\n",{"type":"span","props":{"id":"cb19-2","children":[{"type":"a","props":{"href":"10-sampling.html#cb19-2","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6476},"           ",{"type":"span","props":{"className":"at","children":"power ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6477}," .",{"type":"span","props":{"className":"dv","children":"8"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6478},", "]},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6479},"\n",{"type":"span","props":{"id":"cb19-3","children":[{"type":"a","props":{"href":"10-sampling.html#cb19-3","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6480},"           ",{"type":"span","props":{"className":"at","children":"sig.level ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6481}," .",{"type":"span","props":{"className":"dv","children":"05"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6482},","]},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6483},"\n",{"type":"span","props":{"id":"cb19-4","children":[{"type":"a","props":{"href":"10-sampling.html#cb19-4","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6484},"           ",{"type":"span","props":{"className":"at","children":"type ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6485}," ",{"type":"span","props":{"className":"st","children":"\"two.sample\""},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6486},", "]},"key":6,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6487},"\n",{"type":"span","props":{"id":"cb19-5","children":[{"type":"a","props":{"href":"10-sampling.html#cb19-5","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6488},"           ",{"type":"span","props":{"className":"at","children":"alternative ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6489}," ",{"type":"span","props":{"className":"st","children":"\"two.sided\""},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6490},")"]},"key":8,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6491}]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6492}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6493}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6494},"\n",{"type":"p","props":{"children":["But it is also possible to use this same function to compute the power achieved at a combination of ",{"type":"span","props":{"className":"math inline","children":"\\(n\\)"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6495}," and ",{"type":"span","props":{"className":"math inline","children":"\\(d\\)"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6496},", for example."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6497},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":"Setting the sample size for a replication study has been a persistent issue in the meta-science literature. Naïvely speaking, it seems like you should be able to compute the effect size for the original study and then simply use that as the basis for a classical power analysis."},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6572},"\n",{"type":"p","props":{"children":["This naïve approach has several flaws, however. First, the effect size from the original published paper is likely an overestimate of the true effect size due to publication bias ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-nosek2021","role":"doc-biblioref","children":"Brian A. Nosek et al. 2021"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6573},")"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6574},".",{"type":"label","props":{"htmlFor":"tufte-sn-157","className":"margin-toggle sidenote-number","children":"157"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6575},{"type":"input","props":{"type":"checkbox","id":"tufte-sn-157","className":"margin-toggle","children":null},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6576},{"type":"span","props":{"className":"sidenote","children":[{"type":"span","props":{"className":"sidenote-number","children":"157"},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6577}," Imagine the counterfactual world in which the same study had yielded a smaller and perhaps non-significant effect size. Would it still have been published in the same form, and would you still want to replicate? If not, then consider the fact that the authors “got lucky” – even if the effect is truly present, perhaps this particular experiment happened to observe a larger effect than the true value just by chance."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6578}," Second, the power analysis will only yield the sample size at which the replication will have a particular chance of rejecting the null at some criterion. But it’s quite possible that the original experiment could be ",{"type":"span","props":{"className":"math inline","children":"\\(p\u003c.05\\)"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6579},", the replication could be ",{"type":"span","props":{"className":"math inline","children":"\\(p>.05\\)"},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6580},", ",{"type":"em","props":{"children":"and"},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6581}," 3) the original experiment and the replication results are not significantly different from each other. So a statistically significant replication of the original effect size is not necessarily what you want to aim for."]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6582},"\n",{"type":"p","props":{"children":"Faced with these issues, a replication sample size can be planned in several other ways. First, replicators can use standard strategies above such as SESOI or resource-based planning to rule out large effects, either with high probability or within a known amount of time or money. If the SESOI is high or limited resources are allocated, these strategies can produce an inconclusive result, however. A conclusive answer can require a very substantial commitment of resources."},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6583},"\n",{"type":"p","props":{"children":["Second, ",{"type":"span","props":{"className":"citation","children":["Simonsohn (",{"type":"a","props":{"href":"#ref-simonsohn2015","role":"doc-biblioref","children":"2015"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6584},")"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6585}," recommends the “small telescopes” approach. The idea is not to test whether there ",{"type":"em","props":{"children":"is"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6586}," an effect, but rather where there is an effect ",{"type":"em","props":{"children":"large enough that the original study could have detected it"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6587},". The analogy is to astronomy. If a birdwatcher points their binoculars at the sky and claims to have discovered a new planet, we want to ask not just whether there is a planet at that location, but also whether there is any possibility that they could have seen it using binoculars – if not, perhaps they are right but for the wrong reasons! Simonsohn shows that, if a replicator collects 2.5 times as large a sample as the original, they have 80% power to detect any effect that was reasonably detectable by the original. This simple rule of thumb provides one good starting place for conservative replication studies."]},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6588},"\n",{"type":"p","props":{"children":["Finally, replicators can make use of sequential Bayesian analysis, in which they attempt to gather substantial evidence relative to the support for ",{"type":"span","props":{"className":"math inline","children":"\\(H_1\\)"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6589}," ",{"type":"em","props":{"children":"or"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6590}," ",{"type":"span","props":{"className":"math inline","children":"\\(H_0\\)"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6591},". Sequential bayes is an appealing option because it allows for efficient collection of data that reflects whether an effect is likely to be present in a particular sample, especially in the face of the sometimes prohibitively large samples necessary for SESOI or “small telescopes” analyses."]},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6592},"\n"],"className":"box","data-box":"depth","data-title":"Sample sizes for replication studies"}},{"name":"Box","props":{"children":["\n",{"type":"ol","props":{"style":{"listStyleType":"decimal"},"children":["\n",{"type":"li","props":{"children":{"type":"p","props":{"children":"We want to understand human cognition generally, but do you think it is a more efficient research strategy to start by studying certain features of cognition (perception, for example) in WEIRD convenience populations and then later check our generalizations in non-WEIRD groups? What are the arguments against this efficiency-based strategy?"},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6600}},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6601},"\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["One alternative position regarding sampling is that the most influential experiments aren’t generalizations of some number to a population; they are demonstration experiments that show that some particular effect is possible under some circumstances (think Milgram’s conformity studies, see Chapter ",{"type":"a","props":{"href":"4-ethics.html#ethics","children":"4"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6602},"). On this argument, the specifics of population sampling are often secondary. Do you think this position makes sense?"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6603}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6604},"\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6605},"\n","\n",{"type":"ol","props":{"start":"3","style":{"listStyleType":"decimal"},"children":["\n",{"type":"li","props":{"children":["One line of argument says that we can’t ever make generalizations about the human mind because so much of the historical human population is simply inaccessible to us (we can’t do experiments on ancient Greek psychology). In other words, sampling from a particular population is ",{"type":"em","props":{"children":"also"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6606}," sampling a particular moment in time. How should we qualify our research interpretations to deal with this issue?"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6607},"\n"]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6608},"\n"],"className":"box","data-box":"discussion_questions"}},{"name":"Box","props":{"children":["\n",{"type":"ul","props":{"children":["\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["The original polemic article on the WEIRD problem: Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The WEIRDest people in the world? ",{"type":"em","props":{"children":"Behavioral and Brain Sciences,"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6610}," 33(2-3), 61-83."]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6611}},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6612},"\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["A very accessible introduction to power analysis from its originator: Cohen, J. (1992) A power primer. ",{"type":"em","props":{"children":"Psychological Bulletin,"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6613}," 112, 155-9."]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6614}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6615},"\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":6616},"\n"],"className":"box","data-box":"readings"}}]}}</script></body></html>
