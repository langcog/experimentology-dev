<!DOCTYPE html>
<html lang xmlLang><head><link rel="stylesheet" type="text/css" href="/experimentology-dev/assets/static/index.page.client.1718a224.css"><link rel="preload" href="/experimentology-dev/assets/static/SourceSansPro-Regular.71d10a86.ttf" as="font" type="font/ttf" crossorigin><meta charSet="utf-8"/><meta name="generator" content="pandoc"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta property="og:title" content="Chapter 7 Models | Experimentology"/><meta property="og:type" content="book"/><meta name="author" content="Michael C. Frank, Mika Braginsky, Julie Cachia, Nicholas Coles, Tom E. Hardwicke, Robert Hawkins, Maya Mathur, and Rondeline Williams"/><script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><meta name="description" content="Chapter 7 Models | Experimentology"/><title>Chapter 7 Models | Experimentology</title><link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet"/><link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet"/><link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet"/><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet"/><style type="text/css">code{white-space: pre;}</style><style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style><style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style></head><body>



<div class="row">
<div class="col-sm-12">
<div island></div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="models" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Models</h1>
<div island></div>
<p>In the previous two chapters, we introduced concepts surrounding estimation of an experimental effect and inference about its relationship to the effect in the population. The tools we introduced there are for fairly specific research questions, and so are limited in their applicability. Once you get beyond the world of two-condition experiments in which each participant contributes one data point from a continuous measure, the simple <span class="math inline">\(t\)</span>-test is not sufficient.</p>
<p>In some statistics textbooks, the next step would be to present a whole host of other statistical tests that are designed for other special cases. We could even show a decision-tree: you have repeated measures? Use Test X! Or categorical data? Use Text Y! Or three conditions? Use Test Z! But this isn’t a statistics book, and even if it were, we don’t advocate that approach. The idea of finding a specific narrowly-tailored test for your situation is part and parcel of the dichotomous NHST approach that we tried to talk you out of in the last chapter. If all you want is your <span class="math inline">\(p&lt;.05\)</span>, then it makes sense to look up the test that can allow you to compute a <span class="math inline">\(p\)</span> value in your specific case. But we prefer an approach that is more focused on getting a good estimate of the magnitude of the causal effect.</p>
<p>In this chapter, we begin to explore how to select an appropriate <strong>statistical model</strong> to clearly and flexibly reason about these effects. A statistical model is a way of writing down a set of assumptions about how particular data are generated, the <strong>data generating process</strong>. Statistical models are the bread and butter tools for estimating particular <strong>parameters</strong> of interest from empirical data – like the magnitude of a causal effect associated with an experimental manipulation.</p>
<p>For example, suppose you watch someone tossing a coin and observe a sequence of heads and tails. A simple statistical model might assume that the observed data are generated via the flip of a weighted coin. From the perspective of the last two chapters, we could estimate a standard error for the estimated proportion of flips that are heads (e.g., for 6 heads out of 8 flips, we have <span class="math inline">\(\hat{p}= 0.75 \pm 0.17\)</span>), or we could compare the observed proportion against a null hypothesis. From a model-based perspective, however, we instead begin by thinking about where the data came from: we might assume the coin being flipped has some weight (a <strong>latent</strong>, or unobservable, parameter of the data generating process), and our goal is to determine the most likely value of that weight given the observed data. This single unified model can then also be used to make inferences about whether the coin’s weight differs from some null model (a fair coin, perhaps), or to predict future flips.</p>
<p>This example sounds a lot like the kinds of simple inferential tests we talked about in the previous chapter; not very “model-y.” But things get more interesting when there are multiple parameters to be estimated, as in many real-world experiments. In the tea-tasting scenario we’ve belabored over the past two chapters, a real experiment might involve multiple people tasting different types of tea in different orders, all with some cups randomly assigned to be milk-first or tea-first. What we’ll learn to do in this chapter is to make a model of this situation that allows us to reason about the magnitude of the milk-order effect while also estimating variation due to different people, orders, and tea types. This is the advantage of using models: once you are able to reason about estimation and inference in model-based terms, you will be set free from long decision trees and will be able to flexibly make the assumptions that make sense for your data.<label for="tufte-sn-90" class="margin-toggle sidenote-number">90</label><input type="checkbox" id="tufte-sn-90" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">90</span> We won’t explore the connection to DAGs and Bayesian models here, but one way to think of this model building is as creating a causal theory of the experiment. This approach, which is advocated by <span class="citation">McElreath (<a href="#ref-mcelreath2018" role="doc-biblioref">2018</a>)</span>, creates powerful connections between the ideas about theory we presented in Chapters <a href="1-experiments.html#experiments">1</a> and <a href="7-models.html#models">7</a> and the ideas about models here. If this sounds intriguing, we encourage you to go down the rabbit hole!</span></p>
<p>We’ll begin by discussing the ubiquitous framework for building statistical models, <strong>linear regression</strong>.<label for="tufte-sn-91" class="margin-toggle sidenote-number">91</label><input type="checkbox" id="tufte-sn-91" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">91</span> The name regression originally comes from <span class="citation">Galton (<a href="#ref-galton1877" role="doc-biblioref">1877</a>)</span>’s work on heredity. He was looking at the relationship between the heights of parents and children. He found that children’s heights <em>regressed</em>, and he did so by creating a <em>regression model</em>. Now we use the term “regression” to mean any model of this form.</span> We will then build connections between regression and the <span class="math inline">\(t\)</span>-test. This section will discuss how to add covariates to regression models, and when linear regression does and doesn’t work. In the following section, we’ll discuss the <strong>generalized linear model</strong>, an innovation that allows us to make models of a broader range of data types, including <strong>logistic regression</strong>. We’ll then briefly introduce <strong>mixed models</strong>, which allow us to model clustering in our datasets (such as clusters of observations from a single individual or single stimulus item). We’ll end with some opinionated practical advice on model building.</p>
<p>If you’re interested in building up intuitions about statistical model building, then we recommend reading this chapter all the way through. On the other hand, if you are already engaged in data analysis and want to see an example, we suggest that you skip to the last section, where we give some opinionated practical advice on model building and provide a worked example of fitting a mixed effects model and interpreting it in context.</p>
<div id="regression-models" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Regression models</h2>
<p>There are many types of statistical models, but this chapter will focus primarily on regression, a broad and extremely flexible class of models. A regression model relates a dependent variable to one or more independent variables. Dependent variables are sometimes called <strong>outcome variables</strong>, and independent variables are sometimes called <strong>predictor variables</strong>, <strong>covariates</strong>, or <strong>features</strong>.<label for="tufte-sn-92" class="margin-toggle sidenote-number">92</label><input type="checkbox" id="tufte-sn-92" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">92</span> The reverse is not true – not every predictor or covariate is an independent variable! One of the tricky things about relating regression models to causal hypotheses is that, just because something is on the right side of a regression equation, that doesn’t mean it’s a causal manipulation. And of course, just because you’ve got an estimate of some predictor in a regression, that doesn’t mean the estimate tells you about the magnitude of the <em>causal</em> effect. It could, but it also might not!</span> We will see that many common statistical estimators (like the sample mean) and methods of inference (like the <span class="math inline">\(t\)</span>-test) are actually simple regression models. Understanding this point will help you see many statistical methods as special cases of the same underlying framework, rather than as unrelated, <em>ad hoc</em> tests.</p>
<div id="regression-for-estimating-a-simple-treatment-effect" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Regression for estimating a simple treatment effect</h3>
<p>Let’s start with one of these special cases, namely estimating a treatment effect, <span class="math inline">\(\beta\)</span>, in a two-group design. In Chapter <a href="5-estimation.html#estimation">5</a>, we solved this exact challenge for the tea-tasting experiment. We applied a classical model in which the milk-first ratings are assumed to be normally distributed with mean <span class="math inline">\(\theta_{\text{&lt;}} = \theta_{\text{T}} + \beta\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.<label for="tufte-sn-93" class="margin-toggle sidenote-number">93</label><input type="checkbox" id="tufte-sn-93" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">93</span> Here’s a quick reminder that “model” here is a way of saying “set of assumptions about the data generating procedure.” So saying that some equation is a “model” is the same as saying that we think this is where the data came from. We can “turn the crank” – generate data through the process that’s specified in those equations, e.g., pulling numbers from a normal distribution with mean <span class="math inline">\(\theta_{\text{M}}\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. In essence, we’re committing to the idea that this process will give us data that are substantively similar to the ones we have already.</span></p>
<p>Let’s now write that model as a regression model, that is, as a model that predicts each participant’s tea rating, <span class="math inline">\(Y_i\)</span>, given that participant’s treatment assignment, <span class="math inline">\(X_i\)</span>. <span class="math inline">\(X_i=0\)</span> represents the control (milk-first) group and <span class="math inline">\(X_i=1\)</span> represents the treatment (tea-first) group.<label for="tufte-sn-94" class="margin-toggle sidenote-number">94</label><input type="checkbox" id="tufte-sn-94" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">94</span> Using 0 and 1 is known as <strong>dummy coding</strong>, and allows us to interpret the parameter as the difference of the treatment group (tea-first) from the baseline (milk-first). There are many other ways to code categorical variables, with other interpretations. As a practical tip, be careful to check how your variables are coded before reporting anything!</span> Here, <span class="math inline">\(Y_i\)</span> is the dependent variable, and <span class="math inline">\(X_i\)</span> is the independent variable. The subscripts <span class="math inline">\(i\)</span> index the participants. To make this concrete, you can see some sample tea-tasting data (the first six observations from each condition) below (Table <a href="7-models.html#tab:models-datatable">7.1</a>), with the index <span class="math inline">\(i\)</span>, the condition and its predictor <span class="math inline">\(X_i\)</span>, and the rating <span class="math inline">\(Y\)</span>.</p>
<p><span class="marginnote shownote"><span id="tab:models-datatable">Table 7.1: </span>Example tea tasting data.</span></p>
<table class="table" style="width:auto !important;"><thead><tr><th style="text-align:right;">
id
</th><th style="text-align:left;">
condition
</th><th style="text-align:right;">
X
</th><th style="text-align:right;">
rating (Y)
</th></tr></thead><tbody><tr><td style="text-align:right;">
1
</td><td style="text-align:left;">
milk first
</td><td style="text-align:right;">
0
</td><td style="text-align:right;">
6
</td></tr><tr><td style="text-align:right;">
2
</td><td style="text-align:left;">
milk first
</td><td style="text-align:right;">
0
</td><td style="text-align:right;">
4
</td></tr><tr><td style="text-align:right;">
3
</td><td style="text-align:left;">
milk first
</td><td style="text-align:right;">
0
</td><td style="text-align:right;">
5
</td></tr><tr><td style="text-align:right;">
4
</td><td style="text-align:left;">
milk first
</td><td style="text-align:right;">
0
</td><td style="text-align:right;">
5
</td></tr><tr><td style="text-align:right;">
5
</td><td style="text-align:left;">
milk first
</td><td style="text-align:right;">
0
</td><td style="text-align:right;">
5
</td></tr><tr><td style="text-align:right;">
6
</td><td style="text-align:left;">
milk first
</td><td style="text-align:right;">
0
</td><td style="text-align:right;">
4
</td></tr><tr><td style="text-align:right;">
7
</td><td style="text-align:left;">
tea first
</td><td style="text-align:right;">
1
</td><td style="text-align:right;">
1
</td></tr><tr><td style="text-align:right;">
8
</td><td style="text-align:left;">
tea first
</td><td style="text-align:right;">
1
</td><td style="text-align:right;">
5
</td></tr><tr><td style="text-align:right;">
9
</td><td style="text-align:left;">
tea first
</td><td style="text-align:right;">
1
</td><td style="text-align:right;">
3
</td></tr><tr><td style="text-align:right;">
10
</td><td style="text-align:left;">
tea first
</td><td style="text-align:right;">
1
</td><td style="text-align:right;">
1
</td></tr><tr><td style="text-align:right;">
11
</td><td style="text-align:left;">
tea first
</td><td style="text-align:right;">
1
</td><td style="text-align:right;">
3
</td></tr><tr><td style="text-align:right;">
12
</td><td style="text-align:left;">
tea first
</td><td style="text-align:right;">
1
</td><td style="text-align:right;">
5
</td></tr></tbody></table>
<p>Let’s write this model more formally as a <strong>linear regression of Y on X</strong>. Conventionally, regression models are written with “<span class="math inline">\(\beta\)</span>’’ symbols for all parameters, so we’ll now use <span class="math inline">\(\beta_0 = \theta_M\)</span> for the mean in the milk-first group and <span class="math inline">\(\beta_1 = \theta_T - \theta_M\)</span> as the average difference between the tea-first and milk-first groups.<label for="tufte-sn-95" class="margin-toggle sidenote-number">95</label><input type="checkbox" id="tufte-sn-95" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">95</span> This <span class="math inline">\(\beta\)</span> is a generalization of the one were using to denote the causal effect above and in the previous two chapters.</span></p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]</span></p>
<p>The term <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span> is called the <strong>linear predictor</strong>, and it describes the expected value of an individual’s tea rating, <span class="math inline">\(Y_i\)</span>, given that participant’s treatment group <span class="math inline">\(X_i\)</span> (the single independent variable in this model). That is, for a participant in the control group (<span class="math inline">\(X_i=0\)</span>), the linear predictor is just equal to <span class="math inline">\(\beta_0\)</span>, which is indeed the mean for the control group that we specified above. On the other hand, for a participant in the treatment group, the linear predictor is equal to <span class="math inline">\(\beta_0 + \beta_1\)</span>, which is the mean for the treatment group that we specified. In regression jargon, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <strong>regression coefficients</strong>, where <span class="math inline">\(\beta_1\)</span> represents the association of the independent variable <span class="math inline">\(X\)</span> with the outcome <span class="math inline">\(Y\)</span>.</p>
<p>The term <span class="math inline">\(\epsilon_i\)</span> is the <strong>error term</strong>, referring to random variation of participants’ ratings around the group mean.<label for="tufte-sn-96" class="margin-toggle sidenote-number">96</label><input type="checkbox" id="tufte-sn-96" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">96</span> Formally, we’d write <span class="math inline">\(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\)</span>. The tilde means “is distributed as”, and what follows is a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>.</span> Note that this is a very specific kind of “error”; it does not include “error” due to bias, for example. Instead, you can think of the error terms as capturing the “error” that would be associated with predicting any given participant’s rating based on just the linear predictor. If you predicted a control group participant’s rating as <span class="math inline">\(\beta_0\)</span>, that would be a good guess – but you still expect the participant’s rating to deviate somewhat from <span class="math inline">\(\beta_0\)</span> (i.e., due to variability across participants beyond what is captured by their treatment groups). In our regression model, the linear predictor and error terms together say that participants’ ratings scatter randomly (in fact, normally) around their group means with standard deviation <span class="math inline">\(\sigma\)</span>. And that is exactly the same model we posited in Chapter <a href="5-estimation.html#estimation">5</a>.<label for="tufte-sn-97" class="margin-toggle sidenote-number">97</label><input type="checkbox" id="tufte-sn-97" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">97</span> You may be wondering why so much effort was put into building boutique solutions for these special cases when a unified framework was available the whole time. A partial answer is that the classical infrastructure of statistics was developed before computers were widespread, and these special cases were chosen because they were easy to work with “analytically” (meaning to work out all the math with pen-and-paper, using values from big numerical tables). Now that we have computers with more flexible algorithms, the model-based perspective is more practical and accessible than it used to be.</span></p>

<div class="figure"><span style="display:block;" id="fig:models-ols-plot"></span>
<p class="caption marginnote shownote">
Figure 7.1: (left) Best-fitting regression coefficients for the tea-tasting experiment. (right) Much worse coefficients for the same data. Dotted lines: residuals. Circles: data points for individual participants.
</p>
<img src="experimentology_files/figure-html/models-ols-plot-1.png" alt="(left) Best-fitting regression coefficients for the tea-tasting experiment. (right) Much worse coefficients for the same data. Dotted lines: residuals. Circles: data points for individual participants." width="\linewidth"/>
</div>
<p>Now we have the model. But how do we estimate the regression coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>? The usual method is called <strong>ordinary least squares (OLS)</strong>. Here’s the basic idea. For any given regression coefficient estimates <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>, we would obtain different <strong>predicted values</strong>, <span class="math inline">\(\widehat{Y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 X_i\)</span> for each participant. Some regression coefficient estimates will yield better predictions than others. OLS estimation is designed to find the values of the regression coefficients that optimize these predictions, meaning that the predictions are as close as possible to participants’ true outcomes, <span class="math inline">\(Y_i\)</span>.<label for="tufte-sn-98" class="margin-toggle sidenote-number">98</label><input type="checkbox" id="tufte-sn-98" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">98</span> Specifically, OLS minimizes squared error loss, in the sense that it will choose the regression coefficient estimates whose predictions minimize <span class="math inline">\(\sum_{i=1}^n \left( Y_i - \widehat{Y}_i\right)^2\)</span>, where <span class="math inline">\(n\)</span> is the sample size. A wonderful thing about OLS is that those optimal regression coefficients (generically termed <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span>) turn out to have a very simple closed form solution: <span class="math inline">\(\widehat{\mathbf{\beta}} = \left( \mathbf{X}'\mathbf{X} \right)^{-1} \mathbf{X}'\mathbf{y}\)</span>. We are using more general notation here that supports multiple independent variables: <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> is a vector, <span class="math inline">\(\mathbf{X}\)</span> is a matrix of independent variables for each subject, and <span class="math inline">\(\mathbf{y}\)</span> is a vector of participants’ outcomes. As more good news, the standard error for <span class="math inline">\(\widehat{\mathbf{\beta}}\)</span> has a similarly simple closed form!</span></p>
<p>Figure <a href="7-models.html#fig:models-ols-plot">7.1</a> illustrates the tea tasting data for each condition (the dots) along with the model predictions for each condition <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_0 + \beta_1\)</span> (blue lines). The gap between each data point and the corresponding predictions (the thing that OLS wants to minimize) is shown by the dotted lines. These distances are sample estimates, called <strong>residuals</strong>, of the true errors (<span class="math inline">\(\epsilon_i\)</span>). The left-hand plot shows the OLS coefficient values – the ones that move the model’s predictions as close as possible to the data points, in the sense of minimizing the total squared length of the dashed lines. The right-hand plot shows a substantially worse set of coefficient values.</p>
<div island></div>
<p>You’ll notice that we aren’t talking much about <span class="math inline">\(p\)</span>-values in this chapter. Regression models can be used to produce <span class="math inline">\(p\)</span>-values for specific coefficients, representing inferences about the likelihood of the observed data under some null hypothesis regarding the coefficients. You can also compute Bayes Factors for specific regression coefficients, or use Bayesian inference to fit these coefficients under some prior expectation about their distribution. We won’t talk much about this, or more generally how to fit the models we describe. As we said, we’re not going to give a full treatment of all the relevant statistical topics. Instead we want to help you begin thinking about making models of your data.</p>
</div>
<div id="adding-predictors" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Adding predictors</h3>
<p>The regression model we just wrote down is the same model that underlies the <span class="math inline">\(t\)</span>-test from Chapter <a href="6-inference.html#inference">6</a>. But the beauty of regression modeling is that much more complex estimation problems can also be written as regression models that extend the model we made above. For example, we might want to add another predictor variable, such as the age of the participant.<label for="tufte-sn-99" class="margin-toggle sidenote-number">99</label><input type="checkbox" id="tufte-sn-99" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">99</span> The ability to estimate multiple coefficients at once is a huge strength of regression modeling, so much so that sometimes people use the label <strong>multiple regression</strong> to denote that there is more than one predictor + coefficient pair.</span></p>
<p>Let’s add this new independent variable and a corresponding regression coefficient to our model:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2}  + \epsilon_i
\]</span></p>
<p>Now that we have multiple independent variables, we’ve labeled them <span class="math inline">\(X_{1}\)</span> (treatment group) and <span class="math inline">\(X_{2}\)</span> (age) for clarity.</p>
<p>To illustrate how to interpret the regression coefficients in this model, let’s use the linear predictor to compare the model’s predicted tea ratings for two hypothetical participants who are both in the treatment group: 20-year-old Alice and 21-year old Bob. Alice’s linear predictor tells us that her expected rating is <span class="math inline">\(\beta_0 + \beta_1 + \beta_2 \cdot 20\)</span>. In contrast, Bob’s linear predictor is <span class="math inline">\(\beta_0 + \beta_1 + \beta_2 \cdot 21\)</span>. We could therefore calculate the expected difference in ratings for 21-year-olds versus 20-year olds by subtracting Alice’s linear predictor from Bob’s, yielding just <span class="math inline">\(\beta_2\)</span>.</p>
<p>We would get the same result if Alice and Bob were instead 50 and 51 years old, respectively. This equivalence illustrates a key point about linear regression models in general:</p>
<blockquote>
<p>The regression coefficient represents the expected difference in outcome when comparing any two participants who differ by 1 unit of the relevant independent variable, and who do not differ on any other independent variables in the model.</p>
</blockquote>
<p>Here, the coefficient compares participants who differ by 1 year of age. In “Practical modeling considerations” below, we discuss whether and when to “control for” additional variables (i.e., when to add them to your model).</p>
</div>
<div id="interactions" class="section level3" number="7.1.3">
<h3><span class="header-section-number">7.1.3</span> Interactions</h3>
<p>In our running example, we now have two predictors: condition and age. But what if the effect of condition varies depending on the age of the participant? This situation would correspond to a case where (say) older people were more sensitive to tea ordering, perhaps because of their greater tea experience. We call this an <strong>interaction</strong> effect: the effect of one predictor depends on the state of another.</p>
<p>Interaction effects are easily accommodated in our modeling framework. We simply add a term to our model that is the product of condition (<span class="math inline">\(X_1\)</span>) and age (<span class="math inline">\(X_2\)</span>), and weight this product by another beta, which represents the strength of this interaction:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2}  + \beta_3 X_{i1} X_{i2}  + \epsilon_i
\]</span>
Statistical interactions are a very powerful modeling tool that can help us understand the relationship between different experimental manipulations or between manipulations and covariates (such as age). We discuss their role in experimental design – as well as some of the interpretive challenges that they pose – in much more detail in Chapter <a href="9-design.html#design">9</a>.<label for="tufte-sn-100" class="margin-toggle sidenote-number">100</label><input type="checkbox" id="tufte-sn-100" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">100</span> We won’t go into this topic here, but we do want to provide a pointer to one of the most persistent challenges that come up when you specify regression models with categorical predictors – and especially their interactions: how you “code” these categorical predictors. Above we created a variable <span class="math inline">\(X\)</span> that encoded milk-first tea as 0 and tea-first tea as 1. This coding system is known as <strong>dummy coding</strong>, because <span class="math inline">\(X\)</span> is a “dummy” variable that stands in for milk-first tea. Dummy variables are very easy to think about, but in models with interactions, they can cause some problems. Because the interaction in our example model is a product of the dummy-coded condition variable and age, the interaction term <span class="math inline">\(\beta_3\)</span> is interpreted as the effect of age <em>for the tea-first condition</em> (<span class="math inline">\(X=1\)</span>) and hence the effect of age <span class="math inline">\(\beta_2\)</span> is actually the effect of age <em>for the milk-first condition</em>. The way to deal with this issue is to use a different coding system, such as <strong>contrast coding</strong>. <span class="citation">Davis (<a href="#ref-davis2010" role="doc-biblioref">2010</a>)</span> gives a good tutorial on this tricky topic.</span></p>
</div>
<div id="when-does-linear-regression-work" class="section level3" number="7.1.4">
<h3><span class="header-section-number">7.1.4</span> When does linear regression work?</h3>
<p>Linear regression modeling with OLS is an incredibly powerful technique for creating models to estimate the influence of multiple predictors on a single dependent variable. In fact, OLS is in a mathematical sense the <em>best</em> way to fit a linear model!<label for="tufte-sn-101" class="margin-toggle sidenote-number">101</label><input type="checkbox" id="tufte-sn-101" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">101</span> There is a precise sense in which OLS gives the <em>very best</em> predictions we could ever get from any model that posits linear relationships between the independent variables and the outcome. That is, you can come up with any other linear, unbiased model you want, and yet if the assumptions of OLS are fulfilled, predictions from OLS will always be less noisy than those of your model. This is because of an elegant mathematical result called the Gauss-Markov Theorem.</span> But OLS only “works” – in the sense of yielding good estimates – if three big conditions are met.</p>
<ol style="list-style-type:decimal;">
<li><strong>The predictor relationships being modeled must be linear.</strong> In our comparison of Alice’s and Bob’s expected outcomes based on their 1-year age difference, we were able to interpret the coefficient <span class="math inline">\(\beta_2\)</span> as the average difference in <span class="math inline">\(Y_i\)</span> when comparing participants who differ by 1 year of age, <em>regardless</em> of whether those ages are 20 vs. 21 or 50 vs. 51. But that’s not always true: plenty of things vary <strong>non-linearly</strong> with age – for example, imagine growth in height over age! Linear regression will give bad answers in such cases.<label for="tufte-sn-102" class="margin-toggle sidenote-number">102</label><input type="checkbox" id="tufte-sn-102" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">102</span> One way to accommodate <strong>non-linearities</strong> is to modify the linear predictor to include polynomial terms, such as <span class="math inline">\(X_2^2\)</span>, which then allow us to fit a curve rather than just a straight line. It is always a good idea to use visualizations like scatter plots to look for possible problems with linearity.</span></li>
</ol>

<ol start="2" style="list-style-type:decimal;">
<li><strong>Errors must be independent.</strong> In our example, observations in the regression model (i.e., rows in the dataset) were sampled independently: each participant was recruited independently to the study and each performed a single trial. On the other hand, suppose we have repeated-measures data in which we sample participants, and then obtained multiple measurements for each participant. Within each participant, measurements would likely be correlated (perhaps because participants differ on their general level of tea enjoyment). This correlation can invalidate inferences from a model that does not accommodate the correlation. We’ll discuss this problem in detail below.</li>
</ol>



<ol start="3" style="list-style-type:decimal;">
<li><strong>Errors must be normally distributed and unrelated to the predictor.</strong> Imagine older people have very consistent tea-ordering preferences while younger people do not. In that case, the models’ error term would be less variable for older participants than younger ones. This issue is called <strong>heteroskedasticity</strong>. It is a good idea to plot each independent variable versus the residuals to see if the residuals are more variable for certain values of the independent variable than for others.</li>
</ol>
<p></p>

<p>If any of these three conditions are violated, it can undermine the estimates and inferences you draw from your model.</p>
</div>
</div>
<div id="generalized-linear-models" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Generalized linear models</h2>
<p>So far we have considered continuous outcome measures, like tea ratings. What if we instead had a binary outcome, such as whether a participant liked or didn’t like the tea, or a count outcome, such as the number of cups a participant chose to drink? These and other non-continuous outcomes often violate the assumptions of OLS, in particular because they often induce heteroskedastic errors.</p>
<p>Binary outcomes inherently produce heteroskedastic errors because the variance of a binary variable depends directly on the outcome probability. Errors will be more variable when the outcome probability is closer to 0.50, and much less variable for when the probability is closer to 0 or 1.<label for="tufte-sn-103" class="margin-toggle sidenote-number">103</label><input type="checkbox" id="tufte-sn-103" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">103</span> Specifically, the variance of a binary variable with probability <span class="math inline">\(p\)</span> is simply <span class="math inline">\(p(1-p)\)</span>, which is largest when <span class="math inline">\(p=0.50\)</span>.</span> This heteroskedasticity in turn means that inferences from the model (e.g., <span class="math inline">\(p\)</span>-values) can be incorrect; sometimes just a little bit off but sometimes dramatically incorrect.<label for="tufte-sn-104" class="margin-toggle sidenote-number">104</label><input type="checkbox" id="tufte-sn-104" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">104</span> OLS can also be used with binary outcomes, in which case the coefficients represent differences in probabilities. However, the usual model-based standard errors will be incorrect.</span></p>
<p>Happily, <strong>generalized linear models</strong> (GLMs) are regression models closely related to OLS that can handle non-continuous outcomes. These models are called “generalized” because OLS is one of many members of this large class of models. To see the connection, let’s first write an OLS model more generally in terms of what it says about the expected value of the outcome, which we notate as <span class="math inline">\(E[Y_i]\)</span>:</p>
<p><span class="math display">\[
E[Y_i] = \beta_0 + \sum_{j=1}^p \beta_j X_{ij}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of independent variables, <span class="math inline">\(\beta_0\)</span> is the intercept, and <span class="math inline">\(\beta_j\)</span> is the regression coefficient for the <span class="math inline">\(j^{th}\)</span> independent variable. This equation is just a math-y way of saying that you predict from a regression model by adding up each of the predictors’ contributions to the expected outcome (<span class="math inline">\(\beta_j X_{ij}\)</span>).</p>
<p>The linear predictor of a GLM (i.e., <span class="math inline">\(\beta_0 + \sum_{j=1}^p \beta_j X_{ij}\)</span>) looks exactly the same as for OLS, but instead of modeling <span class="math inline">\(E[Y_i]\)</span>, a GLM models some <strong>transformation</strong>, <span class="math inline">\(g(.)\)</span>, of the expectation:</p>
<p><span class="math display">\[
g( E[Y_i] ) = \beta_0 + \sum_{j=1}^p \beta_j X_{ij}
\]</span></p>
<p>GLMs involve transforming the <em>expectation</em> of the outcome, not the outcome itself! That is, in GLMs, we are not just taking the outcome variable in our dataset and transforming it before fitting an OLS model, but rather we are fitting a different model entirely, one that posits a fundamentally different relationship between the predictors and the expected outcomes. This transformation is called the <strong>link function</strong>. In other words, to fit different kinds of outcomes, all we need to do is construct a standard linear model and then just transform its output via the appropriate link function.</p>
<p>Perhaps the most common link function is the <strong>logit</strong> link, which is suitable for binary data. This link function looks like this, where <span class="math inline">\(w\)</span> is any probability that is strictly between 0 and 1:</p>
<p><span class="math display">\[g(w) = \log \left( \frac{w}{1 - w} \right)\]</span></p>
<p>The term <span class="math inline">\(w / (1 - w)\)</span> is called the <strong>odds</strong> and represents the probability of an event occurring divided by the probability of its not occurring. The resulting model is called <strong>logistic regression</strong> and looks like:</p>
<p><span class="math display">\[
\text{logit}( E[Y_{it}] ) = \log \left( \frac{ E[Y_i] }{1 - E[Y_i] } \right) = \beta_0 + \sum_{j=1}^p \beta_j X_{ij}
\]</span></p>
<p>Exponentiating the coefficients (i.e., <span class="math inline">\(e^{\beta}\)</span>) would yield <strong>odds ratios</strong>, which are the <em>multiplicative</em> increase in the odds of <span class="math inline">\(Y_i=1\)</span> that is associated with a one-unit increase in the relevant predictor variable.</p>
<div class="figure"><span style="display:block;" id="fig:models-logistic-ex"></span>
<p class="caption marginnote shownote">
Figure 7.2: An example of how logistic regression transforms a change in the mean-centered predictor X into a change in the expected outcome Y. The same absolute change in X is associated in a large difference in the probability of the outcome when X is near its mean (blue) vs. a small change in the outcome when X is large (red) or small.
</p>
<img src="experimentology_files/figure-html/models-logistic-ex-1.png" alt="An example of how logistic regression transforms a change in the mean-centered predictor X into a change in the expected outcome Y. The same absolute change in X is associated in a large difference in the probability of the outcome when X is near its mean (blue) vs. a small change in the outcome when X is large (red) or small." width="\linewidth"/>
</div>
<p>Figure <a href="7-models.html#fig:models-logistic-ex">7.2</a> shows the way that a logistic regression model transforms a predictor (<span class="math inline">\(X\)</span>) into an outcome probability that is bounded at 0 and 1. Critically, although the predictor is still linear, the logit link means that the same change in <span class="math inline">\(X\)</span> can result in a different change in the absolute probability of <span class="math inline">\(Y\)</span> depending on where you are on the <span class="math inline">\(X\)</span> scale. In this example, if you are in the middle of the predictor range, a one-unit change in <span class="math inline">\(X\)</span> results in a 0.24 change in probability (blue). At a higher value, the change is much smaller (0.02). Notice how this is different from the linear regression model above, where the same change in age always resulted in the same change in preference!</p>
<div island></div>
<p>We have only scratched the surface of GLMs here. First, there are many different link functions that are suitable for different outcome types. And second, GLMs differ from OLS not only in their link functions, but also in how they handle the error terms. Our broader goal in this chapter is to show you how regression models are <em>models of data</em>. In that context, GLMs use link functions as a way to make models that generate many different times types of outcome data.<label for="tufte-sn-105" class="margin-toggle sidenote-number">105</label><input type="checkbox" id="tufte-sn-105" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">105</span> We sometimes think of linear models as a set of tinker toys you can snap together to stack up a set of predictors. In that context, link functions are an extra “attachment” that you can snap onto your linear model to make it generate a different response type.</span></p>
</div>
<div id="linear-mixed-effects-models" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Linear mixed effects models</h2>
<p>Experimental data often contain multiple measurements for each participant (so-called <strong>repeated measures</strong>). In addition, these measurements are often based on a sample of stimulus items (which then each have multiple measures as well). This clustering is problematic for OLS models, because the error terms for each datapoint are not independent.</p>

<p>Non-independence of datapoints may seem at first glance like a small issue, but it can present a deep problem for making inferences. Take the tea-tasting data we looked at above, where we had 24 observations in each condition. If we fit an OLS model, we observe a highly significant tea-first effect. Here is the estimate and confidence interval for that coefficient: <span class="math inline">\(b = -2.42\)</span>, 95% CI <span class="math inline">\([-3.50, -1.33]\)</span>. Based on what we talked about in the previous chapter, it seems like we’d be licensed in rejecting the null hypothesis that this effect is due to sampling variation and interpret this instead as evidence for a generalizable difference in tea preference in our sampled population.</p>
<p>But suppose we told you that all of those 48 total observations (24 in each condition) were from one individual named George. That would change the picture considerably. Now we’d have no idea whether the big effect we observed reflected a difference in the population, but we would have a very good sense of what George’s preference is!<label for="tufte-sn-106" class="margin-toggle sidenote-number">106</label><input type="checkbox" id="tufte-sn-106" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">106</span> We discuss the strengths and weaknesses of repeated-measures designs like this in Chapter <a href="9-design.html#design">9</a> and the statistical tradeoffs of having many people with a small number of observations per person vs. a small number of people with many observations per person in Chapter <a href="10-sampling.html#sampling">10</a>.</span> The confidence intervals and p-values from our OLS model would be wrong now because all of the error terms would be highly correlated – they would all reflect George’s preferences.</p>
<p>How can we make models that deal with clustered data? There are a number of widely-used approaches for solving this problem including <strong>linear mixed effects models</strong>, <strong>generalized estimating equations</strong>, and <strong>clustered standard errors</strong> (often used in economics). Here we will illustrate how the problem gets solved in linear mixed models, which are an extension of OLS models that are fast becoming a standard in many areas of psychology <span class="citation">(<a href="#ref-bates2014" role="doc-biblioref">Bates et al. 2014</a>)</span>.</p>
<div id="modeling-random-variation-in-clusters" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Modeling random variation in clusters</h3>
<p>In linear mixed effects models, we modify the linear predictor itself to model differences across clusters. Instead of just measuring George’s preferences, suppose we modified the original tea-tasting experiment (without the age covariate) to collect ten ratings from each participant: five milk-first and five tea-first. We define the model the same way as we did before, with some minor differences:</p>
<p><span class="math display">\[
Y_{it} = \beta_0 + \beta_1 X_{it} + \gamma_i + \epsilon_{it}
\]</span></p>
<p>where <span class="math inline">\(Y_{it}\)</span> is participant <span class="math inline">\(i\)</span>’s rating in trial <span class="math inline">\(t\)</span> and <span class="math inline">\(X_{it}\)</span> is the participant’s assigned treatment in trial <span class="math inline">\(t\)</span> (i.e., milk-first or tea-first).</p>
<p>If you compare this equation to the OLS equation above, you will notice that we added two things. First, we’ve added subscripts that distinguish trials from participants. But the big one is that we added <span class="math inline">\(\gamma_i\)</span>, a separate intercept value for each participant. We call this a <strong>random intercept</strong> because it varies across participants (who are randomly selected from the population).<label for="tufte-sn-107" class="margin-toggle sidenote-number">107</label><input type="checkbox" id="tufte-sn-107" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">107</span> Formally, we’d notate this random variation by saying that <span class="math inline">\(\gamma_i \sim N(0, \tau^2)\)</span> – in other words, that participants’ random intercepts are sampled from a normal distribution around the shared intercept <span class="math inline">\(\beta_0\)</span> with standard deviation <span class="math inline">\(\tau\)</span>.</span></p>
<p>The random intercept means that we have assumed that each participant has their own typical “baseline” tea rating – some participants overall just like tea more than others – and these baseline ratings are normally distributed across participants. Thus, ratings are correlated within participants because ratings cluster around each participant’s <em>distinct</em> baseline tea rating. This model is better able to block misleading inferences. For example, suppose we only had one participant in each condition (say, George provided 24 milk-first ratings and Alice provided 24 tea-first ratings). If we found higher ratings in one condition, we would be able to attribute this difference to participant-level variation rather than to the treatment.<label for="tufte-sn-108" class="margin-toggle sidenote-number">108</label><input type="checkbox" id="tufte-sn-108" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">108</span> Of course, this would be a terrible experiment! Ideally, we would address this problem upstream in our experiment design; see Chapter <a href="9-design.html#design">9</a>.</span></p>
<p>Following the same logic, we could fit random intercepts for different stimulus items (for example, if we used different types of tea for different trials). We modeled participants as having normally distributed variation, and we can model stimulus variation the same way. Each stimulus item is assumed to produce a particular average outcome (i.e. some teas are tastier than others), with these average outcomes sampled from a normally distributed population.</p>
<div island></div>
</div>
<div id="random-slopes-and-the-challenges-of-mixed-effects-models" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Random slopes and the challenges of mixed effects models</h3>
<p>Linear mixed effects models can be further extended to model clustering of the independent variables’ <em>effects</em> within subjects, not just clustering of average <em>outcomes</em> within subjects. To do so, we can introduce <strong>random slopes</strong> (<span class="math inline">\(\delta_i\)</span>) to the model, which are multiplied by the condition variable <span class="math inline">\(X\)</span> and represent differences across participants in the effect of tea-tasting:</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{it} + \gamma_i + \delta_{i} X_{it} + \epsilon_{it}
\]</span></p>
<p>Just like the random intercepts, these random slopes will be assumed to vary across participants, following a normal distribution.<label for="tufte-sn-109" class="margin-toggle sidenote-number">109</label><input type="checkbox" id="tufte-sn-109" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">109</span> These random slopes and intercepts can be assumed to be independent or correlated with one another, depending on the modeler’s preference.</span></p>
<p>This model now describes random variation in both overall how much someone likes tea <em>and</em> how strong their ordering preference is. Both of these likely do vary in the population and so it seems like a good thing to put these in your model. Indeed under some circumstances, adding random slopes is argued to be very important for making appropriate inferences.<label for="tufte-sn-110" class="margin-toggle sidenote-number">110</label><input type="checkbox" id="tufte-sn-110" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">110</span> There’s lots of debate in the literature about the best random effect structure for mixed effects models. This is a very tricky and technical subject. In brief, some folks argue for so-called <strong>maximal</strong> models, in which you include every random effect that is justified by the design <span class="citation">(<a href="#ref-barr2013" role="doc-biblioref">Barr et al. 2013</a>)</span>. Here that would mean including random slopes for each participant. The problem is that these models can get very complex, and can be very hard to fit using standard software. We won’t weigh in on this topic, but as you start to use these models on more complex experimental designs, it might be worth reading up.</span></p>
<p>On the other hand, the model is much more complicated. When we had a simple OLS model above, we had only two parameters to fit (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>) but now we have those two plus two more, representing the standard deviations of the individual participant intercepts and slopes, plus parameters for each participant and for the condition effect for each participant. So we went from two parameters to 24!<label for="tufte-sn-111" class="margin-toggle sidenote-number">111</label><input type="checkbox" id="tufte-sn-111" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">111</span> Though we should note that these parameters aren’t technically all independent from one another due to the structure of the mixed effect model.</span>
This complexity can lead to problems in fitting the models, especially with very small datasets (where these parameters are not very well-constrained by the data) or very large datasets (where computing all these parameters can be tricky).<label for="tufte-sn-112" class="margin-toggle sidenote-number">112</label><input type="checkbox" id="tufte-sn-112" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">112</span> Many R users may be familiar with the widely-used <code>lme4</code> package for fitting mixed effects models using frequentist tools related to maximum likelihood. Such models can also be fit using Bayesian inference with the <code>brms</code> package, which provides many powerful methods for specifying complex models.</span></p>
<p>More generally, linear mixed effects models are very flexible, and they have become quite common in psychology. But they do have significant limitations. As we discussed, they can be tricky to fit in standard software packages. Further, the accuracy of these models relies on our ability to specify the structure of the random effects correctly.<label for="tufte-sn-113" class="margin-toggle sidenote-number">113</label><input type="checkbox" id="tufte-sn-113" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">113</span> One particularly problematic situation is when the correlation structure of the errors is mis-specified, for example if observations within a participant are more correlated for participants in the treatment group than in the control group; in such cases, mixed model estimates can be substantially biased <span class="citation">(<a href="#ref-bie2021fitting" role="doc-biblioref">Bie et al. 2021</a>)</span>.</span> If we specify an incorrect model, our inferences will be wrong! But it is sometimes difficult to know how to check whether your model is reasonable, especially with a small number of clusters or observations.</p>
<div island></div>
<div island></div>
</div>
</div>
<div id="how-do-you-use-models-to-analyze-data" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> How do you use models to analyze data?</h2>
<p>In the prior parts of this chapter, we’ve described a suite of regression-based techniques – standard OLS, the generalized linear model, and linear mixed effects models – that can be used to model the data resulting from randomized experiments (as well as many other kinds of data). The advantage of regression models over the simpler estimation and inference methods we described in the prior two chapters is that these models can more effectively take into account a range of different kinds of variation including covariates, multiple manipulations, and clustered structure. Further, when used appropriately to analyze a well-designed randomized experiment, regression models can give an unbiased estimate of a causal effect of interest, our main goal in doing experiments.</p>
<p>But – practically speaking – how should go you about building a model for your experiment? What covariates should you include and what should you leave out? There are many ways to use models to explore datasets, but in this section we will try to sketch a default approach for the use of models to estimate causal effects in experiments in the most straightforward way. Think of this as a starting point. We’ll begin this section by giving a set of rules of thumb, then discuss a worked example. Our final subsections will deal with the issues of when you should include covariates in your model and how to check if your result is robust across multiple different model specifications.</p>
<div id="modeling-rules-of-thumb" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Modeling rules of thumb</h3>
<p>Our approach to statistical modeling is to start with a “default model” that is known in the literature as a <strong>saturated model</strong>. The saturated model of an experiment includes the full design of the experiment – all main effects and interactions – and nothing else. If you are manipulating a variable, include it in your model. If you are manipulating two, include them both and their interaction. If your design includes repeated measurements for participants, include a random effect of participant; if it includes experimental items for which repeated measurements are made, include random effect of stimulus.<label for="tufte-sn-114" class="margin-toggle sidenote-number">114</label><input type="checkbox" id="tufte-sn-114" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">114</span> As discussed above, you can also include the “maximal” random effect structure <span class="citation">(<a href="#ref-barr2013" role="doc-biblioref">Barr et al. 2013</a>)</span>, which involves random slopes as well as intercepts – but recognize that you cannot always fit such models.</span></p>
<p>Don’t include lots of other stuff in your default model. You are doing a randomized experiment, and the strength of randomized experiments is that you don’t have to worry about confounding based on the population (see Chapter <a href="1-experiments.html#experiments">1</a>). So don’t put a lot of covariates in your default model – usually don’t put in any!<label for="tufte-sn-115" class="margin-toggle sidenote-number">115</label><input type="checkbox" id="tufte-sn-115" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">115</span> One corollary to having this kind of default perspective on data analysis: When you see an analysis that deviates substantially from the default, these deviations should provoke some questions. If someone drops a manipulation from their analysis, adds a covariate or two, or fails to control for some clustering in the data, did they deviate because of different norms in their sub-field, or was there some other rationale? This line of reasoning sometimes leads to questions about the extent to which particular analytic decisions are post-hoc and driven by the data (in other words, <span class="math inline">\(p\)</span>-hacked). For an example, see the case study in Chapter <a href="11-prereg.html#prereg">11</a>.</span></p>
<p>This default saturated model then represents a simple summary of your experimental results. Its coefficients can be interpreted as estimates of the effects of interest, and it can be used as the basis for inferences about the relation of the experimental effect to the population using either frequentist or Bayesian tools.</p>
<p>Here’s a bit more guidance about this modeling strategy.</p>
<ol style="list-style-type:decimal;">
<li><p><strong>Preregister your model</strong>. If you change your analysis approach after you see your data, you risk <span class="math inline">\(p\)</span>-hacking – choosing an analysis that biases the estimate of your effect of interest. As we discussed in Chapter <a href="3-replication.html#replication">3</a> and as we will discuss in more detail in Chapter <a href="11-prereg.html#prereg">11</a>, one important strategy for minimizing this problem is to <strong>preregister</strong> your analysis.<label for="tufte-sn-116" class="margin-toggle sidenote-number">116</label><input type="checkbox" id="tufte-sn-116" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">116</span> A side benefit of preregistration is it makes you think through whether your experimental design is appropriate – that is, is there actually an analysis capable of estimating the effect you want from the data you intend to collect?</span></p></li>
<li><p><strong>Visualize the model predictions against the observed data</strong>. As we’ll discuss in Chapter <a href="15-viz.html#viz">15</a>, the “default model” for an experiment should go alongside a “default visualization” known as the <strong>design plot</strong> that similarly reflects the full design structure of the experiment and any primary clusters. In visualizing data like those from our tea-tasting experiment, show individual participants’ average ratings (as in Figure <a href="7-models.html#fig:models-ex-viz">7.3</a>). One way to check whether a model fits your data is then to plot it on top of those data. Sometimes this combination of model and data can be as simple as a scatter plot with a regression line. But seeing the model plotted alongside the data can often reveal a mismatch between the two. A model that does not describe the data very well is not a good source of generalizable inferences!</p></li>
</ol>
<p>
<span class="marginnote shownote">
<span style="display:block;" id="fig:models-ex-viz"></span>
<img src="experimentology_files/figure-html/models-ex-viz-1.png" alt="Raw data, means, and confidence intervals for the tea-tasting experiment." width="\linewidth"/>
Figure 7.3: Raw data, means, and confidence intervals for the tea-tasting experiment.
</span>
</p>
<ol start="3" style="list-style-type:decimal;">
<li><strong>Interpret the model predictions</strong>. Once you have a model, don’t just read off the <span class="math inline">\(p\)</span>-values for your coefficients of interest. Walk through the each coefficient, considering how it relates to your outcome variable. For a simple two group design like we’ve been considering, the condition coefficient is the estimate of the causal effect that you intended to measure! Consider its sign, its magnitude, and its precision (standard error or confidence interval).</li>
</ol>
<p>That said, there are some contexts in which it does make sense to depart from the default saturated model. For example, there may be insufficient statistical power to estimate multiple interaction terms, or covariates might be included in the model to help handle certain forms of missing data. The default model simply represents a very good starting point.</p>
</div>
<div id="a-worked-example" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> A worked example</h3>
<p>
<span class="marginnote shownote">
<span style="display:block;" id="fig:models-sgf-stim"></span>
<img src="images/models/sgf.png" alt="Example stimulus materials from Stiller, Goodman, and Frank (2015)." width="\linewidth"/>
Figure 7.4: Example stimulus materials from Stiller, Goodman, and Frank (2015).
</span>
</p>
<p>All this advice may seem abstract, so let’s put it into practice on a simple example. For a change, let’s look at an experiment that’s not about tea tasting. Here we’ll consider data from an experiment testing preschool children’s language comprehension <span class="citation">(<a href="#ref-stiller2015" role="doc-biblioref">Stiller, Goodman, and Frank 2015</a>)</span>. In this experiment, 2–5 year old children saw displays like the one in Figure <a href="7-models.html#fig:models-sgf-stim">7.4</a>. In the experimental condition, a puppet might say, for example, “My friend has glasses! Which one is my friend?” The goal was to measure how many children made the “pragmatic inference” that the puppet’s friend was the face with glasses and <em>no</em> hat.</p>
<p>To estimate the effect, participants were randomly assigned to either the experimental condition or to a control condition in which the puppet had eaten too much peanut butter and couldn’t talk, but they still had to guess which face was his friend. There were also three other types of experimental stimuli (houses, beds, and plates of pasta). Data from this experiment consisted of 588 total observations from 147 children, with all four stimuli presented to each child. The primary hypothesis of this experiment was that that preschool children could make pragmatic inferences by correctly inferring which of the three faces (for example) the puppet was describing.</p>
<div island></div>
<p>All this advice may seem abstract, so let’s put it into practice on a simple example. For a change, let’s look at an experiment that’s not about tea tasting. Here we’ll consider data from an experiment testing preschool children’s language comprehension [<span class="citation">Stiller, Goodman, and Frank (<a href="#ref-stiller2015" role="doc-biblioref">2015</a>)</span>; we also use these data in Appendix <a href="C-tidyverse.html#tidyverse">C</a>]. In this experiment, 2–5 year old children saw displays like the one in Figure <a href="7-models.html#fig:models-sgf-stim">7.4</a>. In the experimental condition, a puppet might say, for example, “My friend has glasses! Which one is my friend?” The goal was to measure how many children made the “pragmatic inference” that the puppet’s friend was the face with glasses and <em>no</em> hat.</p>
<p>To estimate the effect, participants were randomly assigned to either the experimental condition or to a control condition in which the puppet had eaten too much peanut butter and couldn’t talk, but they still had to guess which face was his friend. There were also three other types of experimental stimuli (houses, beds, and plates of pasta). Data from this experiment consisted of 588 total observations from 147 children, with all four stimuli presented to each child. The primary hypothesis of this experiment was that that preschool children could make pragmatic inferences by correctly inferring which of the three faces (for example) the puppet was describing.</p>
<p>This experimental design looks a lot like some versions of our tea-tasting experiment. We have one primary condition manipulation (the puppet provides information versus does not), presented between-participants so that some participants are in the experimental condition and others are in the control condition. Our measurements are repeated within participants across different experimental stimuli. Finally, we have one important, pre-planned covariate: children’s age. Experimental data are plotted in Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>.<label for="tufte-sn-117" class="margin-toggle sidenote-number">117</label><input type="checkbox" id="tufte-sn-117" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">117</span> Our sampling plan for this experiment was actually <strong>stratified</strong> across age, meaning that we intentionally recruited the same number of participants for each one-year age group – because we anticipated that age was highly correlated with children’s ability to succeed in this task. We’ll describe this kind of sampling in more detail in Chapter <a href="10-sampling.html#sampling">10</a>.</span></p>
<div class="figure"><span style="display:block;" id="fig:models-sgf-plot"></span>
<p class="caption marginnote shownote">
Figure 7.5: Data for <span class="citation">Stiller, Goodman, and Frank (<a href="#ref-stiller2015" role="doc-biblioref">2015</a>)</span>. Each point shows a single participant’s proportion correct trials (out of 4 experimental stimuli) plotted by age group, jittered slightly to avoid overplotting. Larger points and associated confidence intervals show mean and 95% confidence intervals for each condition.
</p>
<img src="experimentology_files/figure-html/models-sgf-plot-1.png" alt="Data for @stiller2015. Each point shows a single participant's proportion correct trials (out of 4 experimental stimuli) plotted by age group, jittered slightly to avoid overplotting. Larger points and associated confidence intervals show mean and 95\% confidence intervals for each condition." width="\linewidth"/>
</div>
<p>How should we go about making our default model for this dataset?<label for="tufte-sn-118" class="margin-toggle sidenote-number">118</label><input type="checkbox" id="tufte-sn-118" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">118</span> This experiment was not preregistered, but the paper includes a separate replication dataset with the same analysis.</span> We simply include each of these design factors in a mixed effects model; we use a logistic link function for our mixed effects model (a <strong>generalized linear mixed effects model</strong>) because we would like to predict correct performance on each trial, which is a binary variable. So that gives us an effect of condition and age as a covariate. We further add an interaction between condition and age in case the condition effect varies meaningfully across groups. Finally, we add random effects of participant, <span class="math inline">\(\gamma_i\)</span>, and experimental item, <span class="math inline">\(\gamma_t\)</span>.<label for="tufte-sn-119" class="margin-toggle sidenote-number">119</label><input type="checkbox" id="tufte-sn-119" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">119</span> As discussed above, this is a tricky decision-point; we could very reasonably have added random slopes as well.</span></p>
<p>The resulting model looks like this:</p>
<p><span class="math display">\[
\text{logit}( E[Y_{it}] ) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \gamma_i + \delta_t
\]</span></p>
<p>Let’s break this complex equation down left to right:</p>
<ul>
<li><span class="math inline">\(\text{logit}( E[Y_{it}] )\)</span> says that we are predicting a logistic function of <span class="math inline">\(E[Y_{it}]\)</span> (where <span class="math inline">\(Y_{it}\)</span> indicates whether child <span class="math inline">\(i\)</span> was correct on trial <span class="math inline">\(t\)</span>).</li>
<li><span class="math inline">\(\beta_0\)</span> is the <strong>intercept</strong>, our estimate of the average log-odds (i.e., the log of the odds ratio) of correct responses for participants in the control condition.</li>
<li><span class="math inline">\(\beta_1 X_{i1}\)</span> is the condition predictor. <span class="math inline">\(\beta_1\)</span> represents the change in log-odds associated with being in the experimental condition (the causal effect of interest!), and <span class="math inline">\(X_{i1}\)</span> is an indicator variable that is 1 if child <span class="math inline">\(i\)</span> is in the experimental condition and 0 for the control condition. Multiplying <span class="math inline">\(\beta_1\)</span> by this indicator means that the predictor has the value 0 for participants in the control condition and <span class="math inline">\(\beta_1\)</span> for those in the experimental condition.</li>
<li><span class="math inline">\(\beta_2 X_{i2}\)</span> is the age predictor. <span class="math inline">\(\beta_2\)</span> represents the difference in log-odds associated with one additional year of age for participants int he control condition[The age coefficient is a <strong>simple effect</strong>, meaning it is the effect of age in the control condition only. That’s because we have dummy coded the condition predictor. If we wanted the average age effect (the <strong>main effect</strong>) then we would need to use contrast coding, per the note in the Interactions section above.], and <span class="math inline">\(X_{i2}\)</span> is the age for each participant.<label for="tufte-sn-120" class="margin-toggle sidenote-number">120</label><input type="checkbox" id="tufte-sn-120" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">120</span> We have <strong>centered</strong> our age predictor in this example so that all estimates from our model are for the average age of our participants. Centering is a good practice for modeling continuous predictors because it increases the interpretability of other parts of the model. For example, because age is centered in this model, the intercept <span class="math inline">\(\beta_0\)</span> can be interpreted as the predicted odds of a correct trial for a participant in the control condition at the average age.</span></li>
<li><span class="math inline">\(\beta_3 X_{i1} * X_{i2}\)</span> is the interaction between experimental condition and age. <span class="math inline">\(\beta_3\)</span> represents the difference in log odds (i.e., the log of the odds ratio) that is associated with being one year older <em>and</em> in the experimental condition versus the control condition. This term is multiplied by both each child’s age <em>and</em> the condition indicator <span class="math inline">\(X_i\)</span>.</li>
<li><span class="math inline">\(\gamma_i\)</span> is the random intercept for participant <span class="math inline">\(i\)</span>, capturing individual variation in the odds of success across trials.</li>
<li><span class="math inline">\(\gamma_t\)</span> is the random intercept for stimulus <span class="math inline">\(t\)</span>, capturing variation in the odds of success across the four different stimuli.</li>
</ul>
<caption>
<span id="tab:models-sgf-model-print">Table 7.2: </span>
</caption>
<div custom-style="Table Caption">
<em>Estimated effects for our generalized linear mixed effects model on data from Stiller et al. (2015).</em>
</div>
<table><thead><tr class="header"><th align="left">Term</th><th align="left"><span class="math inline">\(\hat{\beta}\)</span></th><th align="left">95% CI</th><th align="left"><span class="math inline">\(z\)</span></th><th align="left"><span class="math inline">\(p\)</span></th></tr></thead><tbody><tr class="odd"><td align="left">Control condition</td><td align="left">-1.46</td><td align="left">[-1.88, -1.04]</td><td align="left">-6.76</td><td align="left">&lt; .001</td></tr><tr class="even"><td align="left">Age (years)</td><td align="left">-0.38</td><td align="left">[-0.75, -0.01]</td><td align="left">-1.99</td><td align="left">.046</td></tr><tr class="odd"><td align="left">Expt condition</td><td align="left">2.26</td><td align="left">[1.82, 2.70]</td><td align="left">10.07</td><td align="left">&lt; .001</td></tr><tr class="even"><td align="left">Age (years) * Expt condition</td><td align="left">0.92</td><td align="left">[0.42, 1.43]</td><td align="left">3.60</td><td align="left">&lt; .001</td></tr></tbody></table>
<div island></div>
<p>Let’s estimate this model and see how it looks. We’ll focus here on interpretation of the so-called <strong>fixed effects</strong> (the main predictors), as opposed to the participant and item random effects.<label for="tufte-sn-121" class="margin-toggle sidenote-number">121</label><input type="checkbox" id="tufte-sn-121" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">121</span> Participant means are estimated to have a standard deviation of 0.23 (in log-odds) while items have a standard deviation of 0.27. These indicate that both of our random effects capture meaningful variation.</span> Table <a href="7-models.html#tab:models-sgf-model-print">7.2</a> shows the coefficients. Again, let’s walk through each.</p>

<ul>
<li><p>The <strong>intercept</strong> (control condition estimate) is <span class="math inline">\(\hat{\beta} = -1.46\)</span>, 95% CI <span class="math inline">\([-1.88, -1.04]\)</span>, <span class="math inline">\(z = -6.76\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This estimate reflects that the log-odds of a correct response for an average-age participant in the control condition is -1.46, which corresponds to a probability of 0.19. If we look at Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>, that estimate makes sense: 0.19 seems close to the average for the control condition.</p></li>
<li><p>The <strong>age effect</strong> estimate is <span class="math inline">\(\hat{\beta} = -0.38\)</span>, 95% CI <span class="math inline">\([-0.75, -0.01]\)</span>, <span class="math inline">\(z = -1.99\)</span>, <span class="math inline">\(p = .046\)</span>. This means there is a slight decrease in the log-odds of a correct response for older children in the control condition. Again, looking at Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>, this estimate is interpretable: we see a small decline in the probability of a correct response for the oldest age group.</p></li>
<li><p>The key experimental condition estimate then is <span class="math inline">\(\hat{\beta} = 2.26\)</span>, 95% CI <span class="math inline">\([1.82, 2.70]\)</span>, <span class="math inline">\(z = 10.07\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This estimate means that the log-odds of a correct response for an average-age participant in the experimental condition is the sum of the estimates for the control (intercept) and the experimental conditions: -1.46 + 2.26, which corresponds to a probability of 0.69. Grounding our interpretation in Figure <a href="7-models.html#fig:models-sgf-plot">7.5</a>, this estimate corresponds to the average value for the experimental condition.</p></li>
<li><p>Finally, the <strong>interaction</strong> of age and condition is <span class="math inline">\(\hat{\beta} = 0.92\)</span>, 95% CI <span class="math inline">\([0.42, 1.43]\)</span>, <span class="math inline">\(z = 3.60\)</span>, <span class="math inline">\(p &lt; .001\)</span>. This positive coefficient reflects that with every year of age, the difference between control and experimental conditions grows.</p></li>
</ul>
<p>In sum, this model suggests that there was a substantial difference in performance between experimental and control conditions, in turn supporting the hypothesis that children in the sampled age group can perform pragmatic inferences above chance.</p>
<p>This example illustrates the “default saturated model” framework that we recommend – the idea that a single regression model corresponding to the design of the experiment can yield an interpretable estimate of the causal effect of interest, even in the presence of other sources of variation.</p>
<div island></div>
</div>
<div id="robustness-checks-and-the-multiverse" class="section level3" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Robustness checks and the multiverse</h3>
<p>Using the NHST statistical testing approach that has been common in the psychology literature, even a simple two factor experimental design affords a host of different <span class="math inline">\(t\)</span>-tests and ANOVAs,<label for="tufte-sn-122" class="margin-toggle sidenote-number">122</label><input type="checkbox" id="tufte-sn-122" class="margin-toggle"/><span class="sidenote"><span class="sidenote-number">122</span> Although we don’t cover this point here, ANOVAs are also a special case of regression.</span> offering many opportunities for <span class="math inline">\(p\)</span>-hacking and selective reporting. We’ve been advocating here instead for a “default model” approach in which you pre-plan and pre-register a single regression model that captures the planned features of your experimental design including manipulations and sources of clustering. This approach can help you to navigate some of the complexity of data analysis by having a standard approach that you take in almost every case.</p>
<p>Not every dataset will be amenable to this approach, however. For complex experimental designs or unusual measures, sometimes it can be hard to figure out how to specify or fit the default saturated model. And especially in these cases, the choice of model can make a big difference to the magnitude of the reported effect. To quantify variability in effect size due to model choice, “Many Analysts” projects have asked a set of teams to approach a dataset using different analysis methods. The consistent result from these projects has been that there is substantial variability in outcomes depending on what approach is taken <span class="citation">(<a href="#ref-silberzahn2018" role="doc-biblioref">Silberzahn et al. 2018</a>; <a href="#ref-botvinik-nezer2020" role="doc-biblioref">Botvinik-Nezer et al. 2020</a>)</span>.</p>
<p><strong>Robustness analysis</strong> (also sometimes called “sensitivity analysis” or “multiverse analysis”, which sounds cooler) is a technique for addressing the possibility that an individual analysis over- or under-estimates a particular effect by chance <span class="citation">(<a href="#ref-steegen2016" role="doc-biblioref">Steegen et al. 2016</a>)</span>. The general idea is that analysts explore a space of different possible analyses. In its simplest form, alternative model specifications can be reported in a supplement; more sophisticated versions of the idea call for averaging estimates across a range of possible specifications and reporting this average as the primary effect estimate.</p>
<p>The details of this kind of analysis will vary depending on what you are worried about your model being sensitive to. One analyst might be concerned about the effects of adding different covariates; another might be using a Bayesian framework and be concerned about sensitivity to particular prior values. If you get similar results across many different specifications, you can sleep better at night. The primary principle to take home is a bit of humility about our models. Any given model is likely wrong in some of its details. Investigating the sensitivity of your estimates to the details of your model specification is a good idea.</p>
</div>
</div>
<div id="chapter-summary-models" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Chapter summary: Models</h2>
<p>In the last three chapters, we have spelled out a framework for data analysis that focuses on our key experimental goal: a measurement of a particular causal effect. We began with basic techniques for estimating effects and making inferences about how these effects estimated from a sample can be generalized to a population. This chapter showed how these ideas naturally give rise to the idea of making models of data, which allow estimation of effects in more complex designs. Simple regression models, which are formally identical to other inference methods in the most basic case, can be extended with the generalized linear model as well as with mixed effects models. Finally, we ended with some guidance on how to build a “default model” – an (often pre-registered) regression model that maps onto your experimental design and provides the primary estimate of your key causal effect.</p>
<div island></div>
<div island></div>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-barr2013" class="csl-entry">
Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. <span>“Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.”</span> <em>Journal of Memory and Language</em> 68 (3): 255–78.
</div>
<div id="ref-bates2014" class="csl-entry">
Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2014. <span>“Fitting Linear Mixed-Effects Models Using Lme4.”</span> <em>arXiv Preprint arXiv:1406.5823</em>.
</div>
<div id="ref-bie2021fitting" class="csl-entry">
Bie, Ruofan, Sebastien Haneuse, Nathan Huey, Jonathan Schildcrout, and Glen McGee. 2021. <span>“Fitting Marginal Models in Small Samples: A Simulation Study of Marginalized Multilevel Models and Generalized Estimating Equations.”</span> <em>Statistics in Medicine</em> 40 (24): 5298–5312.
</div>
<div id="ref-botvinik-nezer2020" class="csl-entry">
Botvinik-Nezer, Rotem, Felix Holzmeister, Colin F Camerer, Anna Dreber, Juergen Huber, Magnus Johannesson, Michael Kirchler, et al. 2020. <span>“Variability in the Analysis of a Single Neuroimaging Dataset by Many Teams.”</span> <em>Nature</em> 582 (7810): 84–88.
</div>
<div id="ref-davis2010" class="csl-entry">
Davis, Matthew J. 2010. <span>“Contrast Coding in Multiple Regression Analysis: Strengths, Weaknesses, and Utility of Popular Coding Structures.”</span> <em>Journal of Data Science</em> 8 (1): 61–73.
</div>
<div id="ref-galton1877" class="csl-entry">
Galton, Francis. 1877. <span>“Typical Laws of Heredity.”</span> In. Royal Institution of Great Britain.
</div>
<div id="ref-mancl2001covariance" class="csl-entry">
Mancl, Lloyd A, and Timothy A DeRouen. 2001. <span>“A Covariance Estimator for GEE with Improved Small-Sample Properties.”</span> <em>Biometrics</em> 57 (1): 126–34.
</div>
<div id="ref-mcelreath2018" class="csl-entry">
McElreath, Richard. 2018. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. Chapman; Hall/CRC.
</div>
<div id="ref-montgomery2018" class="csl-entry">
Montgomery, Jacob M, Brendan Nyhan, and Michelle Torres. 2018. <span>“How Conditioning on Posttreatment Variables Can Ruin Your Experiment and What to Do about It.”</span> <em>Am. J. Pol. Sci.</em> 62 (3): 760–75.
</div>
<div id="ref-silberzahn2018" class="csl-entry">
Silberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi, Frederik Aust, Eli Awtrey, Štěpán Bahnı́k, et al. 2018. <span>“Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results.”</span> <em>Advances in Methods and Practices in Psychological Science</em> 1 (3): 337–56.
</div>
<div id="ref-steegen2016" class="csl-entry">
Steegen, Sara, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel. 2016. <span>“Increasing Transparency Through a Multiverse Analysis.”</span> <em>Perspectives on Psychological Science</em> 11 (5): 702–12. <a href="https://doi.org/10.1177/1745691616658637">https://doi.org/10.1177/1745691616658637</a>.
</div>
<div id="ref-stiller2015" class="csl-entry">
Stiller, Alex J, Noah D Goodman, and Michael C Frank. 2015. <span>“Ad-Hoc Implicature in Preschool Children.”</span> <em>Language Learning and Development</em> 11 (2): 176–90.
</div>
</div>
<p style="text-align:center;">
<a href="6-inference.html"><button class="btn btn-default">Previous</button></a>
<a href="8-measurement.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



<script type="module" src="/experimentology-dev/assets/entries/entry-server-routing.47e088c4.js" defer></script><link rel="modulepreload" href="/experimentology-dev/assets/entries/src_index.page.client.882297ec.js" as="script" type="text/javascript"><link rel="modulepreload" href="/experimentology-dev/assets/chunks/chunk-0be53b7a.js" as="script" type="text/javascript"><script id="vite-plugin-ssr_pageContext" type="application/json">{"pageContext":{"_pageId":"/src/index","islands":[{"name":"TOC","props":{"name":"Experimentology: An Open Science Approach to Experimental Psychology Methods","items":[{"name":"Foundations","items":[{"name":"Experiments","href":"1-experiments"},{"name":"Theories","href":"2-theories"},{"name":"Replication","href":"3-replication"},{"name":"Ethics","href":"4-ethics"}]},{"name":"Statistics","items":[{"name":"Estimation","href":"5-estimation"},{"name":"Inference","href":"6-inference"},{"name":"Models","href":"7-models"}]},{"name":"Design","items":[{"name":"Measurement","href":"8-measurement"},{"name":"Design","href":"9-design"},{"name":"Sampling","href":"10-sampling"}]},{"name":"Execution","items":[{"name":"Preregistration","href":"11-prereg"},{"name":"Data collection","href":"12-collection"},{"name":"Project management","href":"13-management"}]},{"name":"Reporting","items":[{"name":"Writing","href":"14-writing"},{"name":"Visualization","href":"15-viz"},{"name":"Meta-analysis","href":"16-meta"},{"name":"Conclusions","href":"17-conclusions"}]},{"name":"Appendices","items":[{"name":"GitHub","href":"A-git"},{"name":"R Markdown","href":"B-rmarkdown"},{"name":"Tidyverse","href":"C-tidyverse"},{"name":"ggplot","href":"D-ggplot"},{"name":"Instructor’s guide","href":"E-instructors"}]}]}},{"name":"Box","props":{"children":["\n",{"type":"ul","props":{"children":["\n",{"type":"li","props":{"children":"Articulate a strategy for estimating experimental effects using statistical models"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":3863},"\n",{"type":"li","props":{"children":"Build intuitions about how classical statistical tests relate to linear regression models"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":3864},"\n",{"type":"li","props":{"children":"Explore variations of the linear model, including generalized linear models and mixed effects models"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":3865},"\n",{"type":"li","props":{"children":"Reason about tradeoffs and strategies for model specification, including the use of control variables"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":3866},"\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":3867},"\n"],"className":"box","data-box":"learning_goals"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["As it turns out, fitting an OLS regression model in R is extremely easy. The underlying call is ",{"type":"code","props":{"children":"lm"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4094},", which stands for linear model. You can fit the model with a single call to this function with a “formula” as its argument. Here’s the call:"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4095},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb13","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":{"type":"span","props":{"id":"cb13-1","children":[{"type":"a","props":{"href":"7-models.html#cb13-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4096},"mod ",{"type":"span","props":{"className":"ot","children":"\u003c-"},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4097}," ",{"type":"span","props":{"className":"fu","children":"lm"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4098},"(rating ",{"type":"span","props":{"className":"sc","children":"~"},"key":6,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4099}," condition, ",{"type":"span","props":{"className":"at","children":"data ="},"key":8,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4100}," tea_data)"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4101}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4102}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4103}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4104},"\n",{"type":"p","props":{"children":["Formulas in R are a special kind of terse notation for regression equations where you write the outcome, ",{"type":"code","props":{"children":"~"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4105}," (distributed as), and the predictors. R assumes that you want an intercept by default, and there are also a number of other handy defaults that make R formulas a nice easy way to specify relatively complex regression models, as we’ll see below."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4106},"\n",{"type":"p","props":{"children":["Once you’ve fit the model and assigned it to a variable, you can call ",{"type":"code","props":{"children":"summary"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4107}," to see a summary of the parameters of the model:"]},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4108},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb14","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":{"type":"span","props":{"id":"cb14-1","children":[{"type":"a","props":{"href":"7-models.html#cb14-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4109},{"type":"span","props":{"className":"fu","children":"summary"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4110},"(mod)"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4111}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4112}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4113}},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4114},"\n",{"type":"p","props":{"children":["You can also extract the coefficient values using ",{"type":"code","props":{"children":"coef(mod)"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4115},", and put them in a handy dataframe using ",{"type":"code","props":{"children":"tidy(mod)"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4116}," from the ",{"type":"code","props":{"children":"broom"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4117}," package."]},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4118},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["GLMs are as easy to fit in R as standard LMs. You simply need to call the ",{"type":"code","props":{"children":"glm"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4272}," function – and to specify the link function. For our example above of a binary “liking” judgment, the call would be:"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4273},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb15","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":{"type":"span","props":{"id":"cb15-1","children":[{"type":"a","props":{"href":"7-models.html#cb15-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4274},{"type":"span","props":{"className":"fu","children":"glm"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4275},"(liked_tea ",{"type":"span","props":{"className":"sc","children":"~"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4276}," condition, ",{"type":"span","props":{"className":"at","children":"data ="},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4277}," tea_data, ",{"type":"span","props":{"className":"at","children":"family ="},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4278}," ",{"type":"span","props":{"className":"st","children":"\"binomial\""},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4279},")"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4280}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4281}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4282}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4283},"\n",{"type":"p","props":{"children":["The ",{"type":"code","props":{"children":"family"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4284}," argument specifies the type of distribution being used, where “binomial” is the logistic link function."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4285},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["Remarkably, GLMMs are not much harder to specify in R than standard LMs. One very popular package is ",{"type":"code","props":{"children":"lme4"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4343}," ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-bates2014","role":"doc-biblioref","children":"Bates et al. 2014"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4344},")"]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4345},", which provides the ",{"type":"code","props":{"children":"lmer"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4346}," and ",{"type":"code","props":{"children":"glmer"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4347}," functions (the latter for generalized linear mixed effect models). For our example here, we’d write:"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4348},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb16","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":[{"type":"span","props":{"id":"cb16-1","children":[{"type":"a","props":{"href":"7-models.html#cb16-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4349},{"type":"span","props":{"className":"fu","children":"library"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4350},"(lme4)"]},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4351},"\n",{"type":"span","props":{"id":"cb16-2","children":[{"type":"a","props":{"href":"7-models.html#cb16-2","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4352},{"type":"span","props":{"className":"fu","children":"lmer"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4353},"(rating ",{"type":"span","props":{"className":"sc","children":"~"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4354}," condition ",{"type":"span","props":{"className":"sc","children":"+"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4355}," (",{"type":"span","props":{"className":"dv","children":"1"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4356}," ",{"type":"span","props":{"className":"sc","children":"|"},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4357}," id), ",{"type":"span","props":{"className":"at","children":"data ="},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4358}," tea_data)"]},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4359}]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4360}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4361}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4362},"\n",{"type":"p","props":{"children":["In this model, the syntax ",{"type":"code","props":{"children":"(1 | id)"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4363}," specifies that we want a random intercept for each level of ",{"type":"code","props":{"children":"id"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4364},"."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4365},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["Specifying random slopes in the ",{"type":"code","props":{"children":"lme4"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4412}," package is also relatively straightforward:"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4413},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb17","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":[{"type":"span","props":{"id":"cb17-1","children":[{"type":"a","props":{"href":"7-models.html#cb17-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4414},{"type":"span","props":{"className":"fu","children":"library"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4415},"(lme4)"]},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4416},"\n",{"type":"span","props":{"id":"cb17-2","children":[{"type":"a","props":{"href":"7-models.html#cb17-2","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4417},{"type":"span","props":{"className":"fu","children":"lmer"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4418},"(rating ",{"type":"span","props":{"className":"sc","children":"~"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4419}," condition ",{"type":"span","props":{"className":"sc","children":"+"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4420}," (condition ",{"type":"span","props":{"className":"sc","children":"|"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4421}," id), ",{"type":"span","props":{"className":"at","children":"data ="},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4422}," tea_data)"]},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4423}]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4424}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4425}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4426},"\n",{"type":"p","props":{"children":["Here, ",{"type":"code","props":{"children":"(condition | id)"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4427}," means “a separate random slope for ",{"type":"code","props":{"children":"condition"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4428}," should be fit for each level of ",{"type":"code","props":{"children":"id"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4429},".” Of course, specifying such a model is easier than fitting it correctly."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4430},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["A second class of methods that helps resolve issues of clustering is ",{"type":"strong","props":{"children":"generalized estimating equations"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4432}," (GEE). In this approach, we leave the linear predictor alone. We do not add random intercepts or slopes, nor do we assume anything about the distribution of the errors (i.e., we no longer assume that they are normal, independent, and homoskedastic)."]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4433},"\n",{"type":"p","props":{"children":["In GEE, we instead provide the model with an initial “guess” about how we think the errors might be related to one another; for example, in a repeated-measures experiment, we might guess that the errors are exchangeable, meaning that they are correlated to the same degree within each participant but are uncorrelated across participants. Instead of ",{"type":"em","props":{"children":"assuming"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4434}," that our guess is correct, as do linear mixed models (LMM), GEE estimates the correlation structure of the errors empirically, using our guess as a starting point, and it uses this correlation structure to arrive at point estimates and inference for the regression coefficients. Remarkably, as the number of clusters and observations become very large, GEE will ",{"type":"em","props":{"children":"always"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4435}," provide unbiased point estimates and valid inference, ",{"type":"em","props":{"children":"even if"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4436}," our guess about the correlation structure was bad. Additionally, with simple finite-sample corrections ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-mancl2001covariance","role":"doc-biblioref","children":"Mancl and DeRouen 2001"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4437},")"]},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4438},", GEE seems to provide valid inference at smaller numbers of clusters than does LMM."]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4439},"\n",{"type":"p","props":{"children":["The price paid for these nice safeguards against model misspecification is that, in principle, GEE will typically have less statistical power than LMM ",{"type":"em","props":{"children":"if"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4440}," the LMM is in fact correctly specified, but the difference may be surprisingly slight in practice ",{"type":"span","props":{"className":"citation","children":["(",{"type":"a","props":{"href":"#ref-bie2021fitting","role":"doc-biblioref","children":"Bie et al. 2021"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4441},")"]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4442},". For these reasons, some of this book’s authors actually favor GEE with finite-sample corrections over LMM as the default model for clustered data, though they are much less common in psychology.\n"]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4443},"\n"],"className":"box","data-box":"depth","data-title":"An alternative approach: Generalized estimating equations"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["If you want to follow along with this example, you’ll have to load the example data and do a little bit of preprocessing (also covered in Appendix ",{"type":"a","props":{"href":"C-tidyverse.html#tidyverse","children":"C"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4511},"):"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4512},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb18","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":[{"type":"span","props":{"id":"cb18-1","children":[{"type":"a","props":{"href":"7-models.html#cb18-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4513},"sgf ",{"type":"span","props":{"className":"ot","children":"\u003c-"},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4514}," ",{"type":"span","props":{"className":"fu","children":"read_csv"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4515},"(",{"type":"span","props":{"className":"st","children":"\"https://raw.githubusercontent.com/langcog/experimentology/main/data/tidyverse/stiller_scales_data.csv\""},"key":6,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4516},") ",{"type":"span","props":{"className":"sc","children":"|>"},"key":8,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4517}]},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4518},"\n",{"type":"span","props":{"id":"cb18-2","children":[{"type":"a","props":{"href":"7-models.html#cb18-2","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4519},"  ",{"type":"span","props":{"className":"fu","children":"mutate"},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4520},"(",{"type":"span","props":{"className":"at","children":"age_group ="},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4521}," ",{"type":"span","props":{"className":"fu","children":"cut"},"key":6,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4522},"(age, ",{"type":"span","props":{"className":"dv","children":"2"},"key":8,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4523},{"type":"span","props":{"className":"sc","children":":"},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4524},{"type":"span","props":{"className":"dv","children":"5"},"key":10,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4525},", ",{"type":"span","props":{"className":"at","children":"include.lowest ="},"key":12,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4526}," ",{"type":"span","props":{"className":"cn","children":"TRUE"},"key":14,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4527},"), "]},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4528},"\n",{"type":"span","props":{"id":"cb18-3","children":[{"type":"a","props":{"href":"7-models.html#cb18-3","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4529},"         ",{"type":"span","props":{"className":"at","children":"condition ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4530}," ",{"type":"span","props":{"className":"fu","children":"factor"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4531},"(",{"type":"span","props":{"className":"fu","children":"ifelse"},"key":6,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4532},"(condition ",{"type":"span","props":{"className":"sc","children":"=="},"key":8,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4533}," ",{"type":"span","props":{"className":"st","children":"\"Label\""},"key":10,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4534},", ",{"type":"span","props":{"className":"st","children":"\"Experimental\""},"key":12,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4535},", ",{"type":"span","props":{"className":"st","children":"\"Control\""},"key":14,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4536},")))"]},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4537}]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4538}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4539}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4540},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":["To fit the model described above, the first step is to prepare your predictors. In this case, we center the ",{"type":"code","props":{"children":"age"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4660}," predictor."]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4661},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb19","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":{"type":"span","props":{"id":"cb19-1","children":[{"type":"a","props":{"href":"7-models.html#cb19-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4662},"sgf",{"type":"span","props":{"className":"sc","children":"$"},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4663},"age_centered ",{"type":"span","props":{"className":"ot","children":"\u003c-"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4664}," ",{"type":"span","props":{"className":"fu","children":"scale"},"key":6,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4665},"(sgf",{"type":"span","props":{"className":"sc","children":"$"},"key":8,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4666},"age, ",{"type":"span","props":{"className":"at","children":"scale="},"key":10,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4667},{"type":"span","props":{"className":"cn","children":"FALSE"},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4668},")"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4669}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4670}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4671}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4672},"\n",{"type":"p","props":{"children":["Again we use the ",{"type":"code","props":{"children":"lme4"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4673}," package, this time with the ",{"type":"code","props":{"children":"glmer"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4674}," function. Again we have to specify our link function, just like in a standard GLM, by choosing the distribution family."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4675},"\n",{"type":"div","props":{"className":"sourceCode","id":"cb20","children":{"type":"pre","props":{"className":"sourceCode r","children":{"type":"code","props":{"className":"sourceCode r","children":[{"type":"span","props":{"id":"cb20-1","children":[{"type":"a","props":{"href":"7-models.html#cb20-1","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4676},"mod ",{"type":"span","props":{"className":"ot","children":"\u003c-"},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4677}," lme4",{"type":"span","props":{"className":"sc","children":"::"},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4678},{"type":"span","props":{"className":"fu","children":"glmer"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4679},"(correct ",{"type":"span","props":{"className":"sc","children":"~"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4680}," age_centered ",{"type":"span","props":{"className":"sc","children":"*"},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4681}," condition ",{"type":"span","props":{"className":"sc","children":"+"},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4682}," (",{"type":"span","props":{"className":"dv","children":"1"},"key":13,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4683},{"type":"span","props":{"className":"sc","children":"|"},"key":14,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4684},"subid) ",{"type":"span","props":{"className":"sc","children":"+"},"key":16,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4685}," (",{"type":"span","props":{"className":"dv","children":"1"},"key":18,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4686},{"type":"span","props":{"className":"sc","children":"|"},"key":19,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4687},"item), "]},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4688},"\n",{"type":"span","props":{"id":"cb20-2","children":[{"type":"a","props":{"href":"7-models.html#cb20-2","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4689},"                   ",{"type":"span","props":{"className":"at","children":"family ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4690}," ",{"type":"span","props":{"className":"st","children":"\"binomial\""},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4691},","]},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4692},"\n",{"type":"span","props":{"id":"cb20-3","children":[{"type":"a","props":{"href":"7-models.html#cb20-3","aria-hidden":"true","tabIndex":"-1","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4693},"                   ",{"type":"span","props":{"className":"at","children":"data ="},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4694}," sgf)"]},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4695}]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4696}},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4697}},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4698},"\n",{"type":"p","props":{"children":["You can see a summary of the fitted model using ",{"type":"code","props":{"children":"summary(mod)"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4699}," as before. The only big difference from ",{"type":"code","props":{"children":"lm"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4700}," is that here you can extract both fixed and random effects (with ",{"type":"code","props":{"children":"fixef(mod)"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4701}," and ",{"type":"code","props":{"children":"ranef(mod)"},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4702}," respectively)."]},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4703},"\n"],"className":"box","data-box":"code"}},{"name":"Box","props":{"children":["\n",{"type":"p","props":{"children":"Let’s come back to one piece of advice that we gave above about making a “default” model of an experiment: not including covariates. This advice can seem surprising. Many demographic factors are of interest to psychologists and other behavioral scientists, and in observational studies these factors will almost always be related to important life outcomes. So why not put them into our experimental models? After all, we did include age in our worked example above!"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4745},"\n",{"type":"p","props":{"children":["Well, if you have one or at most a small handful of covariates that you believe are meaningfully related to the outcome, you ",{"type":"em","props":{"children":"can"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4746}," plan in advance to put them in your model. If you think that your effect is likely to be ",{"type":"strong","props":{"children":"moderated"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4747}," a specific demographic characteristic – as we did with age in our developmental example above – then this inclusion can be quite useful."]},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4748},"\n",{"type":"p","props":{"children":["Further, including covariates can increase the precision of your estimates by reducing “noise” in your outcome, if you hypothesize that they interact. What’s surprising though is how ",{"type":"em","props":{"children":"little"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4749}," this adjustment does to increase your overall precision unless the correlation between covariate and outcome is very strong."]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4750},"\n",{"type":"p","props":{"children":["Figure ",{"type":"a","props":{"href":"7-models.html#fig:models-precision","children":"7.6"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4751}," shows the relationship between estimation error and the inclusion of covariates via a simple simulation. Only when the correlation between covariate and outcome (e.g., age and tea rating) is greater than ",{"type":"span","props":{"className":"math inline","children":"\\(r=0.6\\)"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4752}," to ",{"type":"span","props":{"className":"math inline","children":"\\(r=0.8\\)"},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4753}," does this adjustment really help."]},"key":7,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4754},"\n",{"type":"div","props":{"className":"figure","children":[{"type":"span","props":{"style":{"display":"block"},"id":"fig:models-precision","children":null},"key":0,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4755},"\n",{"type":"p","props":{"className":"caption marginnote shownote","children":"\nFigure 7.6: Decreases in estimation error due to adjusting for covariates, plotted by the N participants in each group and the correlation between the covariate and the outcome.\n"},"key":2,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4756},"\n",{"type":"img","props":{"src":"experimentology_files/figure-html/models-precision-1.png","alt":"Decreases in estimation error due to adjusting for covariates, plotted by the N participants in each group and the correlation between the covariate and the outcome.","width":"\\linewidth","children":null},"key":4,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4757},"\n"]},"key":9,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4758},"\n",{"type":"p","props":{"children":"That said, there are quite a few reasons not to include covariates – motivating our recommendation to skip them in your default model unless you have very strong theory-based expectations for either (A) a correlation with the outcome or (B) a strong moderation relationship."},"key":11,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4759},"\n",{"type":"p","props":{"children":"The first is simply because we don’t need to. Because randomization cuts causal links, our experimental estimate is an unbiased estimate of the causal effect of interest (at least for large samples). We are guaranteed that, in the limit of many different experiments, even though people with different ages will be in the different tea tasting conditions, this source of variation will be averaged out. Actually, including unnecessary covariates into models (slightly) decreases the probability that the model can detect a true effect (that is, it decreases statistical precision and power). Just by chance covariates can “soak up” variation in the outcome, leaving less to be accounted for by the true effect!"},"key":13,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4760},"\n",{"type":"p","props":{"children":["The second reason is that you can actually compromise your causal inference by including some covariates, particularly those that are collected ",{"type":"em","props":{"children":"after"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4761}," randomization. The logic of randomization is that you cut all causal links between features of the sample and the condition manipulation. But you can “uncut” these links by accident by adding variables into your model that are related to group status. This problem is generically called ",{"type":"strong","props":{"children":"conditioning on post-treatment variables"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4762}," and a full discussion of is out of the scope of this book, but it’s something to avoid (and read up on if you’re worried about it; ",{"type":"span","props":{"className":"citation","children":["Montgomery, Nyhan, and Torres (",{"type":"a","props":{"href":"#ref-montgomery2018","role":"doc-biblioref","children":"2018"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4763},")"]},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4764},")."]},"key":15,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4765},"\n",{"type":"p","props":{"children":"Finally, one of the standard justifications for adding covariates – because your groups are unbalanced – is actually ill-founded as well. People often talk about “unhappy randomization”: you randomize to the different tea-tasting groups, for example, but then it turns out the mean age is a bit different between groups. Then you do a t-test or some other statistical test and find out that you actually have a significant age difference. This practice makes no sense! Because you randomized, you know that the difference in ages occurred by chance. Further, incidental demographic differences between groups are unlikely to be important unless that characteristic is highly correlated with the outcome (see above). Instead, if the sample size is small enough that meaningfully large incidental differences could arise in important confounders, then it is preferable to stratify randomization on that confounder at the outset (and then also to include that covariate in modeling)."},"key":17,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4766},"\n",{"type":"p","props":{"children":"So these are our options: if a covariate is known to be very strongly related to our outcome, we can include it in our default model. Otherwise, we avoid a lot of trouble by leaving covariates out."},"key":19,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4767},"\n"],"className":"box","data-box":"depth","data-title":"When does it makes sense to include covariates in a model?"}},{"name":"Box","props":{"children":["\n",{"type":"ol","props":{"style":{"listStyleType":"decimal"},"children":["\n",{"type":"li","props":{"children":{"type":"p","props":{"children":"Choose a paper that you have read for your research and take a look at the statistical analysis. Does the reporting focus more on hypothesis testing or on estimating effect sizes?"},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4793}},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4794},"\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["We focused here on the linear model as a tool for building models, contrasting this perspective with the common “statistical testing” mindset. But – here’s the mind-blowing thing – most of those statistical tests are special cases of the linear model anyway. Take a look at this extended meditation on the equivalences between tests and models: ",{"type":"a","props":{"href":"https://lindeloev.github.io/tests-as-linear/#9_teaching_materials_and_a_course_outline","className":"uri","children":"https://lindeloev.github.io/tests-as-linear/#9_teaching_materials_and_a_course_outline"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4795},". If the paper you chose for question 1 used tests, could their tests be easily translated to models? How would the use of a model-based perspective change the results section of the paper?"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4796}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4797},"\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["Take a look at this cool visualization of hierarchical (mixed effect) models: ",{"type":"a","props":{"href":"http://mfviz.com/hierarchical-models/","className":"uri","children":"http://mfviz.com/hierarchical-models/"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4798},". In your own research, what are the most common units that group together your observations?"]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4799}},"key":5,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4800},"\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4801},"\n"],"className":"box","data-box":"discussion_questions"}},{"name":"Box","props":{"children":["\n",{"type":"ul","props":{"children":["\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["An opinionated practical guide to regression modeling and data description: Gelman, A., Hill, J., & Vehtari, A. (2020). ",{"type":"em","props":{"children":"Regression and other stories"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4803},". Cambridge University Press. Free online at ",{"type":"a","props":{"href":"https://avehtari.github.io/ROS-Examples/","className":"uri","children":"https://avehtari.github.io/ROS-Examples/"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4804},"."]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4805}},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4806},"\n",{"type":"li","props":{"children":{"type":"p","props":{"children":["A more in-depth introduction to the process of developing Bayesian models of data that allow for estimation and inference in complex datasets: McElreath, R. (2020). ",{"type":"em","props":{"children":"Statistical rethinking: A Bayesian course with examples in R and Stan"},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4807},". Chapman and Hall/CRC. Free materials available at ",{"type":"a","props":{"href":"https://xcelab.net/rm/statistical-rethinking/","className":"uri","children":"https://xcelab.net/rm/statistical-rethinking/"},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4808},"."]},"key":"!undefined","ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4809}},"key":3,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4810},"\n"]},"key":1,"ref":"!undefined","__k":null,"__":null,"__b":0,"__e":null,"__d":"!undefined","__c":null,"__h":null,"constructor":"!undefined","__v":4811},"\n"],"className":"box","data-box":"readings"}}]}}</script></body></html>
